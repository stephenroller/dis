\chapter{Introduction}
\label{sec:intro}

In modern Natural Language Processing (NLP) research, there is great deal of
focus on sophisticated semantic tasks which require complex inference and
synthesis of knowledge. These include tasks like Question Answering (QA), where
computers must read and answer questions about passages
\cite{hermann:2015:nips,weston:2016:iclr}, and Recognizing Textual Entailment
(RTE), where computers must decide whether a hypothesis utterance logically
follows (or can be inferred) from a given piece of text
\cite{dagan:2006:mlc,marelli:2014:semeval,bowman:2015:emnlp}. In the future,
these technologies could influence a wide range of industries: from threat
identification in defense, to fact checking in journalism, to
synthesis of knowledge in science and medicine.

Substantial progress has been made in systems which perform
logical inferences in QA and RTE, especially as common benchmarks
and datasets have become available
\cite{dagan:2006:mlc,giampiccolo:2007:pascal,bentivogli:2009:tac,marelli:2014:semeval,bowman:2015:emnlp}.
Yet in most sophisticated, compositional model of semantics, systems must
ultimately consider the semantics of individual lexical items to form a
conclusion. This often requires an understanding about the different
relationships that can occur between lexical items. Consider the following
example:
\begin{quote}
  \label{ex:rte}
  Text (Antecedent): The bright girl reads a book.\\
  Hypothesis (Consequent): A smart child looks at pages of text.
\end{quote}
Any language processing system wishing to infer the second sentence from the
first must know quite a bit of information about these words: it must know that
girl is a kind of child (hypernymy), and that bright and smart have the same
meaning in this context (synonymy); that books contain pages of text
(meronomy), and that reading involves looking at these pages (world knowledge).

Although significant progress has been made on the task of Recognizing Textual
Entailment, many of these systems ultimately depend on some lexical resources
\cite{beltagy:2014:semeval,bjerva:2014:semeval,lai:2014:semeval,marelli:2014:semeval,beltagy:2016:cl}.
Possibly the most famous lexical resource is WordNet \cite{miller:1995:acm},
which organizes the lexicon into a large ontology,
though many other resources also exist and are used
\cite{baker:1998:acl,baroni:2011:gems,baroni:2012:eacl,ganitkevitch:2013:naacl,jurgens:2012:semeval,levy:2014:conll,turney:2015:nle}.
Unfortunately, resources as expansive as WordNet are extremely expensive
to create, and as language is ever-changing, they are inevitably
always incomplete. As such, any dependence on manually constructed resources
represents one weak point in some Natural Language Understanding systems. Even
recent neural network approaches, which attempt to learn entailments without
explicitly depending on these resources, often cannot make entailment
predictions about words which were not in the training data
\cite{bowman:2015:emnlp,cheng:2016:emnlp,pavlick:2016:acl}.

Distributional Semantics offers one potential solution to these issues of lexical
coverage. Distributional Semantics takes inspiration from the famous quote:
``You shall know a word by the company it keeps'' \cite{firth:1957:la}. In
Distributional Semantics, representations of word meaning are automatically
induced by counting or modeling the {\em contexts} in which a word appears.
Distributional Semantics is often called Vector Space Models (VSMs) of
language, because words are represented as vectors a high-dimensional vector space.
Words with similar semantics will have similar vectors in this
space. Since VSMs do not require annotated corpora, they are used and
studied as an alternative or predictor of particular lexical resources
\cite{baroni:2012:eacl,erk:2008:emnlp,turney:2010:jair}.

In our work, we consider how VSMs
can be leveraged to predict some of the lexical inferences necessary in RTE.
Namely, we present techniques and models for predicting specific
lexical relationships, entailments, and substitutions using Distributional
Semantics. In Lexical Relationship Detection, we must
predict whether two words exhibit specific relationships, like hypernymy (is-a
relationships) or meronymy (has-a relationships). This is sometimes supplanted
by Lexical Entailment Detection, where we must predict a coarser entailment
prediction. We present two original models which can learn to predict hypernymy
or general entailment relations
\cite{roller:2014:coling,beltagy:2016:cl,roller:2016:emnlp}, and evaluate their
performance on different datasets. In these works, we also make significant
contributions in experimental setups which prevent issue with lexical
memorization \cite{roller:2014:coling}, and insight into how these models
work \cite{roller:2016:emnlp}.  We also present an original model for Lexical
Substitution, where one must predict a context-specific synonym for a given
target word in a sentential context \cite{roller:2016:naacl}.

Finally, we propose several short- and long-term extensions to our completed
work. In the short-term, we focus mainly on how our more recent work may be
expanded to lexical relationships other than hypernymy, and how our recent
publications may  contribute as a component in an end-to-end RTE system, which
would cement the connection between our work and Textual Entailment. In the
long-term, we propose three broad directions forward: (1) providing better
internal consistency in the predictions made by our system; (2) integration
of larger contexts into our Lexical Substitution model; and (3) improved
models of distributional semantics which more efficiently use syntactic
information.

\section{Thesis Contributions}

\section{Thesis Outline}


