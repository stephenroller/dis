\chapter{Conclusion}

Distributional Semantics has come a long way in its ability to contribute to
difficult Natural Language Processing tasks. In this proposal, we several
methods for distilling {\em lexical knowledge} out of distributional vectors.
We discussed two models of hypernymy detection, which take as input pairs of
words, and predict whether those models stand in a hypernymy relation.  Through
detailed analysis, we have a clear picture of how these models work, and how
they relate to some of the linguistic literature on Lexical Entailment, like
the Distributional Inclusion Hypothesis and Hearst Patterns. We also saw that
at least one of these models is able to positively contribute to an RTE system
We discussed a new model of Lexical
Substitution, which we argue is related to lexical entailment by acting as a
form of {\em in-context synonym prediction}. Our Lexical Substitution model
outperforms some comparable baselines, and analysis shows its improvements
derive predominantly from exploiting simple unigram priors.

We also discussed multiple directions for how our work could be extended
in the future. In the short term, we focus on exploring how our H-features
model could also contribute to the RTE Lexical Entailment classifier, and
whether the H-features model is able to model relations other than hypernymy.
We also discussed several ways in which the Lexical Substitution model could
contribute to the RTE Lexical Entailment Classifier. Progress in these areas
would contribute significantly to our thesis that our models extract useful
lexical information for Recognizing Textual Entailment.

Finally, we discussed different long-term directions for future research. These
ideas are much larger, and less guaranteed than the short-term research
proposals, but also reflect our opinion on where the field should go from here.
Namely, we considered how Hypernymy and Relationship prediction could be
improved by imposing simple constraints inherent to ontologies. We also
discussed several ways in which more context could be exploited in Lexical
Substitution, or how we could remove some of the independence assumptions made
by our current model. We note that improvements could come by either
exploiting linguistic knowledge about how different syntactic relations
interact, or by using recent successful neural network techniques. Lastly, we
considered how current syntactic distributional spaces make an inefficient use
of the available information, and discussed ways they could be extended to be
{\em generative} over syntactic relations, instead of just artificially
limiting the statistics they model. Improvements in these spaces would likely
improve performance of distributional models in many tasks, including
Lexical Entailment and Lexical Substitution.

