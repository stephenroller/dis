Many modern efforts in Natural Language Understanding depend on rich and
powerful semantic representations of words. Systems for sophisticated logical
and textual reasoning often depend heavily on
lexical resources to provide critical information about relationships
between words, but these lexical resources are expensive to create and
maintain, and are never fully comprehensive. Distributional Semantics has
long offered methods for automatically inducing meaning representations from
large corpora, with little or no annotation efforts. The resulting
representations are valuable proxies of semantic similarity, but simply knowing
two words are similar cannot tell us their relationship, or whether one
entails the other.

In this thesis, we consider how methods from Distributional Semantics may be
applied to the difficult task of lexical entailment, where one must predict whether
one word implies another. We approach this by
showing contributions in areas of hypernymy detection, lexical relationship
prediction, and lexical substitution, and textual entailment. We propose novel
experimental setups, models, analysis, and interpretations, which ultimate
provide us with a better understanding of both the nature of lexical entailment,
as well as the information available within distributional representations.


