\chapter{Lexical Entailment in RTE}
\section{Subsystem in complete RTE system (Beltagy et al., 2016)}
\label{sec:rtesubsystem}

Beyond showing that Asym is better able to improve performance on lexical
relationship datasets, we should also show that it can improve performance
in an end-to-end Recognizing Textual Entailment (RTE) system. Specifically, we compare Asym's performance
to a variety of lexical entailment classifiers which use a variety of
hand-engineered features and word lexicons.  These predictions made by lexical
entailment classifiers are used as to generate {\em logical rules}; an inference
engine based on Markov Logic Networks (MLNs) is then used to form predictions
about complete {\em sentential} entailment \cite{beltagy:2016:cl}.

To this end, we employ the Sentences Involving Compositional Knowledge (SICK)
dataset, which contains nearly 10k sentence pairs, evenly split between
training and test sets \cite{marelli:2014:semeval}. Sentence pairs were
extracted randomly image caption datasets, and then simplified and extended to
cover semantic issues like negation and quantifiers, and then manually
annotated as {\em entailing} (the antecedent implies the consequent), {\em
contradicting} (the antecedent implies the opposite of the consequent), or
neutral (neither of the above). Two examples from the dataset are shown in
Table~\ref{tab:sickexample}.

\begin{table}
  \centering
  \begin{tabular}{|ll|}
    \hline
    {\bf Label} & {\bf Antecedent/Consequent}\\
    \hline
    Entailing & A: Two teams are competing in a football match\\
    & C: Two groups of people are playing football\\
    \hline
    Contradicting & A: The brown horse is near a red barrel at the rodeo\\
    & C: The brown horse is far from a red barrel at the rodeo\\
    \hline
  \end{tabular}
  \caption{Example entailing and contradicting sentences from the SICK dataset.}
  \label{tab:sickexample}
\end{table}

To train the lexical entailment classifier, we extract {\em lexical pairs} from
the SICK dataset using a variation on Robinson Resolution \cite{robinson:1965:jacm}. The
full details are outside the scope of this document, but briefly, we employ
an off-the-shelf semantic parser called Boxer, which translates sentences into
First Order Logical formulas \cite{bos:2008:step}. We then use theorem proving
techniques in order to extract lexical rules which {\em must} be true in the dataset.
For example, given that ``The girl talks'' entails ``A girl speaks,'' we can
eliminate {\em girl} and automatically conclude that {\em talks} lexically
entails {\em speaks}.

Crucially, by knowing entailment decisions about the
entire sentences, and by performing unification over their logical structures,
we can use theorem proving to automatically label certain {\em atomic rules} as
entailing, contracting, or neutral. These atomic rules, like {\em talks entails speaks}, can
be interpreted as {\em lexical entailment pairs} for use in a lexical entailment
classifier. Most, but not all, pairs can be labeled automatically, and those
that cannot are manually annotated by two of the authors \cite{beltagy:2016:cl}.
The final result is a novel dataset of lexical entailment pairs, which we call
RRR (Robinson Resolution Rules), which we use to train and compare lexical
entailment classifiers.

We compare the performance of several existing lexical entailment
classifiers which employ a variety of hand-engineered features and lexicons
like WordNet.  Most of the hand-engineered features come from
\newcite{lai:2014:semeval}, and include things like Wordform features (e.g.,
do the words have the same lemma or Part-of-Speech?); WordNet features (e.g.,
are the words hypernyms or co-hyponyms in WordNet?); and Distributional
Similarity (e.g., cosine distance between two words in a distributional space).

We also present a novel extension of the real-valued distributional similarity
features, by {\em binning} cosines into ranges (e.g. 0.0--0.1, \ldots, 0.9--1.0)
to transform them into binary-valued features, after observing that
mid-similar terms (those with a cosine of $\sim.80$, like {\em cat} and {\em
animal}) were more likely entailments than those with high similarity
(cosine of $\sim.95$, like {\em cat} and {\em dog}). We found this binning
technique significantly improved the contribution distributional similarity
in feature-engineered lexical entailment classifiers.

Finally, we also compare to the Asym classifier trained on our new RRR dataset,
and on a variation of Asym which concatenates the Asym features with the
antecedent (LHS) and consequent (RHS) vectors, which we call Asym + Concat,
since it uses both the Asym features and the Concat (LHS+RHS) features. For
both the distributional similarity features, and the Asym models, we use two
distributional spaces: one which uses a BoW window of two words, and one based
on syntactic contexts.

We evaluate all of the lexical entailment classifiers listed above on two
variations of the task: Intrinsic accuracy, and RTE accuracy. In the Intrinsic
setup, the lexical entailment classifiers are evaluated on their performance on
the RRR dataset in a standard 10-fold cross-validation setup. In the RTE setup,
the classifiers are trained on items extracted from the training portion of the
RTE dataset, and then used as the sole source of lexical knowledge in a
complete RTE pipeline \newcite{beltagy:2013:starsem,beltagy:2016:cl}
to predict the test portion of the RTE dataset. This ensures we are actually
testing whether Asym is able to contribute in a rich RTE pipeline.

Table~\ref{tab:evallexical} shows performance of the various lexical entailment
classifiers on each of our evaluation settings. We also report a degenerate
baseline (always guess Neutral), and an upper baseline, which always gives gold
lexical entailment decisions. Note that this upper baseline on the RTE
task is {\em not} 100\%, due to a mixture of issues like parsing errors,
inference errors, and other issues in the RTE inference engine.

\begin{table}
\centering
\begin{tabular}{|lrr|}
    \hline
    {\bf Feature set} & {\bf Intrinsic} & {\bf RTE Test}\\
    \hline
    Always guess neutral & 56.6 & 69.3 \\
    Gold standard annotations&100.0 & 94.6 \\
    \hline
    Wordform only        & 57.4 & 70.4 \\
    Dist. Sim. only      & 68.8 & 76.7 \\
    WordNet only         & 79.1 & 84.2 \\
    Asym only            & 76.8 & 79.2 \\
    Asym + Concat        & 81.4 & 82.6 \\
    \hline
    All features         & 84.6 & 83.8 \\
    \hline
\end{tabular}
\caption{Accuracies for various Lexical Entailment classifiers on the RRR (Intrinsic) and SICK (RTE) datasets.}
\label{tab:evallexical}
\end{table}

In general, we find that Asym performs relatively well, even compared some of
the hand-engineered features proposed by \newcite{lai:2014:semeval}, indicating
that Asym is flexible and able to learn beyond just the LEDS and BLESS datasets.
Unsurprisingly, the classifiers which use only Wordform features or only
distributional similarity both perform much worse than the information-rich
WordNet features, or the more sophisticated Asym classifier.
We also notice that the classifier which combines Asym with the Concat
vectors performs substantially better than Asym does by itself, indicating
a need for additional study, which we will address in
Section~\ref{sec:hfeatures}.

Unsurprisingly, we find that the WordNet classifier does a bit better than the
others, which is expected given that WordNet is an information-rich resource,
and contains gold annotations about word relationships, rather than
distributional information.  Finally, we observe a classifier which combines
all of these features together does the best on the Intrinsic accuracy, but not
as strongly as WordNet on the end-to-end task; some of this can be attributed
to a handful of systematic differences between the training and test sets of
SICK.

We also perform a qualitative analysis to compare the Distributional Similarity
classifier with the Asym classifier. We manually inspect and compare the
predictions and errors made by each, and find that Asym does substantially
better at distinguishing hypernymy from co-hyponymy.  This is what we had hoped
to find, given the findings in Section~\ref{sec:asym}, and that cosine is known
to heavily favor co-hyponymy \cite{baroni:2011:gems}.  However, we also find
that cosine features are better at discovering synonymy, and that Asymmetric
frequently mistakes antonymy as an entailing. Additional qualitative analyses
comparing models are available in \newcite{beltagy:2016:cl}.

\section{Proposed: H-features Model for RTE}

The results of Section~\ref{sec:rtesubsystem} showed that the Asym
model can provide improvements in a complete, end-to-end RTE
system, especially when combined with traditionally hand-engineered features.
However, we have not yet considered whether our new H-features model also
contributes to an end-to-end RTE pipeline.
We suspect, given its substantial improvements over Asym in lexical datasets,
that it will also be able to be able to improve the end-to-end system.

It remains unclear what is the best way to integrate it with the
additional hand-engineered features used in the pipeline. One possibility
would be to simply use the H-feature detector procedure to provide additional
information to the lexical entailment classifier, but there are complications
in that the H-feature detector demands nonlinear models, while the
hand-engineered features prefer linear classifiers. We suspect this
may be alleviated with careful hyperparameter tuning.

These proposed experiments will also need to be tightly coupled with the
complete non-hypernymy detector discussed in the previous section. If H-features
are useful useful for predicting co-hyponymy or meronomy, it also stands to
reason that they should be useful in the final RTE subsystem. We will need to
explore the best form of combination with both kept in mind.


