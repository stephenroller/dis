\chapter{Conclusion}
\label{ch:conclusion}

Distributional Semantics has come a long way in its ability to contribute
to difficult Natural Language Processing tasks. Distributional representations
of word meaning have been successfully used on a wide variety of lexical semantics
tasks, and have become the shoulders on which modern NLP methods stand
\cite{goldberg:2016:jair}.

In this dissertation, we considered how distributional representations of word
meaning can be useful for identifying and exploiting {\em lexical entailment}.
We have considered a variety of challenging tasks related to the area,
including hypernymy detection, lexical relationship prediction, and lexical
substitution.

In hypernymy detection and lexical relationship prediction, we predict
whether a given pair of words exhibits a particular linguistic relationship,
such as hypernymy, co-hyponymy, or meronomy.
Our work has shown that the choice of experimental setup is critical to properly
understanding how methods may or may not generalize to novel lexical items,
and introduces the notion of lexical memorization. We proposed evaluating models
in an adversarial setup with zero lexical overlap between training and test
sets, allowing us to measure generalization to unseen words, and our
setup has become a standard evaluation in the literature.

We also proposed Asym, a new model of hypernymy detection and
lexical relationship prediction. We analyzed Asym to identify
its relationship to existing linguistic theories of hypernymy, like the
Distributional Inclusion Hypothesis. We showed that our model does not suffer
from some of the prototypicality limitations of other models proposed in the
literature, which only make predictions based on how similar a pair is to
relationship prototypes, without regard for actual relationship between the
words in a pair.

We considered the behavior of a model known to exhibit {\em only}
prototypicality behavior, and proposed a novel analysis method to interpret the
behavior of the model. By interpretting a model's hyperplane in terms of {\em
context} space, we observed that simpler models actually
learn to identify H-features, or Hearst pattern like contexts which are most
indicative of hypernymy. Building on this observation and the success of
Asym, we proposed a novel model which identifies and exploits multiple sets of
H-features through an iterative PCA-like procedure. Our model matches or
exceeds the performance of other models in the literature on several datasets.
We also extended our H-feature model so that it may predict several lexical
relationships simultaneously and outperform existing models in the literature.
We examined the H-features learned for the non-hypernymy relationships and
found strong evidence for additional known Hearst patterns in the literature,
as well as many alternative patterns highly indicative of target relationships.

We considered a novel model of lexical substitution, where one must predict
a synonym for a polysemous word in a particular sentential context. We introduced
a modification to a simple model from he literature, and showed
large performance gains over several datasets and evaluations compared to the
baseline model. Our performance most improved on the difficult task of
substitution generation, where a one may propose any word from the entire
vocabulary as a substitute. Additional analysis showed that our model improved
over prior work via integration of a unigram frequency bias, allowing
it to discard rare or mispelled substitutes from its predictions.

Finally, we considered how each of the contributions above could be integrated
into a real, end-to-end system for Recognizing Textual Entailment. We considered
the role of a lexical entailment classifier in an RTE system, and the
relationship between distribuitonal similarity and the probability of lexical
entailment. We found that each of our components, on its own, was able to
perform competitively or outperform a number of baseline lexical entailment
features. Furthermore, we showed that the components may be combined with
existing lexical entailment features to improve overall peformance,
significantly outperforming a system which used only a fixed, high-quality
lexical resource. Our contributing components also combine together to produce
a model which outperforms any component on its own.

In short, this thesis has shown that Distributional Semantics can contribute
significantly to difficult lexical and textual entailment tasks through a
variety of techniques and models, and that efforts on each task have yielded
a deeper understanding about distributional word representations and the
information contained within them.


\section{Future Work}

As with all areas of research, the work in this thesis answers some questions,
but it also raises new ones. In this section, we briefly consider some possible
future directions of research.

\paragraph{Long-distance Dependency Contexts}
Throughout this thesis, we saw that a variety of syntactic contexts can play
important roles in the predictive power of models. For example, in the work on
H-features, we saw that some complex syntactic contexts, like
\ctx{nmod:between\depinv+cross} was one context learned to be indicative of
co-hyponymy, and \ctx{nmod:from\depinv+handcraft} was learned to be indicative
of constructive material. These contexts were extracted due to the collapsed
dependency structures used at the time of space creation
\cite{marneffe:2008:techreport}. The dependencies that are collapsed are
based on a small list of fixed, English multiword expressions. As such,
other, more complex syntactic fragments, or fragments with multiple
intermediary points may form additional strong signal for the applications
explored in this thesis. Models that use {\em paths} through dependency
parses have had success in lexical relationship prediction
\cite{shwartz:2016:acl,shwartz:2016:cogalex2} and semantic role labeling
\cite{roth:2016:acl}. Models of these paths are likely useful as proxies for
wider-context information in a future distributional models.

\paragraph{Multi-relation Lexicalization}
In a similar vein, current work on distributional models treats each
co-occurrence as an independent, isolated event in the corpus. For example, in
our distributional spaces, we model a verb-subject co-occurrences independently
of verb-object co-ocurrences independently of verb-preposition co-occurrences.
In some cases, it may be better to model a word's co-occurrences jointly. For
example, if someone ``kills two birds,'' we may have an industrious hunter, and
if we observe someone ``killing with one stone,'' we may recall the story of
David and Goliath. However, if we observe someone ``killing two birds with one
stone,'' the situation is entirely different from the previous two scenarios.
It follows that some words or specific co-occurrences may be better modeled
if two co-occurring {\em contexts} are also modeled as a single unit, rather
than separate, smaller contexts.
Indeed, \newcite{chersoni:2016:emnlp} find that jointly modeling a verb using
its subject {\em and} object gives better estimates of human similarity
scores, but we suspect joint modeling may be valuable for several combinations
of syntactic relationships.

Sparsity will always be a fundamental challenge in such joint-modeling
applications, but clustering methods alieve this problem by using coarser
co-occurrences for modeling.  \newcite{melamud:2014:conll} makes a step in this
direction by using probability estimates of a language model to estimate joint
co-occurrence, but they ignore explicit markers of syntactic relations.
Clustering approaches may also be helpful if used as substitutes for words
occurring in particular relations, giving coarser groups of co-occurrences.
However, they may result in the loss of idiomatic constructions, like our example, or
cause overgeneralization from idioms, such as ``murdering two animals with one
rock.''

\paragraph{Joint H-feature Learning}

In Chapter~\ref{ch:hpm}, we used an iterative, progressive procedure for
extracting multiple H-features, with the second H-feature only being
extracted after the first is individually modeled and subtracted from the data.
However, this greedy procedure possibly results in capturing less-than-ideal H-features.
In our own examples, we saw that the H-features related to
vehicles were mixed with H-features of common tools, though finding two
separate groups of H-features is likely better. One solution to this problem would be
to learn to extract all H-features {\em simultaneously} and {\em jointly},
rather than using our iterative greedy procedure. This could be done using a
neural network model, such that the first layer's weight matrix
correspond to each separate H-feature. In preliminary experiments testing this
approach, we struggled with overfitting on specific lexical items, and suffered
a great deal of the same lexical memorization issue of early models in the
area. It may be necessary to regularize the model using orthogonal
regularization \cite{brock:2017:iclr}, or a sparsity regularizer on hidden
activations.

\paragraph{Lexical Relationship Generation}

In the broader context, predicting whether two words exhibit a lexical
relationship is of limited use when one or both sides of the pair
is unconstrained, and the best possible word must be {\em generated} or
{\em selected} from the vocabulary. For example, some applications may be
actually more interested in asking ``what is the hypernym of \lit{cat}?'' rather
than ``is (\lit{cat},~\lit{animal}) hypernymy?'' At the present moment, our
H-feature model and similar lexical relationship models could only answer the
former question by enumerating over the vocabulary, and querying the model word
by word. Naturally, even if we improved our predictive accuracy substantially
beyond current state-of-the-art, the the numerous trials will result
in a large number of false positives.

Furthermore, we saw in our own experiments that highly imbalanced datasets
(like the Medical dataset) have much lower accuracies than more balanced
datasets, indicating the problem cannot be solved via data augmentation alone.
As such, distributional lexical relationship {\em generation} is an interesting
area of further research with limited prior work
\cite{fu:2014:acl,nayak:2015:techreport,espinosaanke:2016:emnlp}. We hope
future researchers in this area will benefit from the lessons learned in this
thesis.

\paragraph{Structured Relationship Prediction}

In the same vein as Lexical Relationship Generation, we should also recall that
many interesting linguistic relationships are actually interconnected,
structured problems. For example, co-hyponymy is a useful linguistic
relationship with its own interesting signals and Hearst patterns (as we saw in
Chapter~\ref{ch:hpm}), but it is also inseparably defined in terms of
hypernymy. Yet, the models discussed in this thesis do not consider relationship
prediction in this manner: (\lit{cat}, \lit{animal}) is classified independently
of (\lit{dog}, \lit{animal}) and (\lit{cat}, \lit{dog}). A good system of
lexical relationship prediction, or generation, should weigh the evidence for
all three pairs together, and come to a conclusion as a whole. That is, models
should also be forced to consider the natural constraints of the taxonomic
properties, in addition to evidence between two individual pairs.
\newcite{snow:2006:acl} showed that evidence from different relations can
be weighted and combined to produce correctly structured taxonomies, but an
ideal work would bake the structure into the original model.

\paragraph{Soft Alignments in Lexical Entailment}

In our Lexical Entailment classifier of Chapter~\ref{ch:rte}, we used a
hard alignment procedure in order to model entailment aspects of the
non-head words in phrases \newcite{lai:2014:semeval}. Although this
successfully models some entailing and nonentailing phrases, the greedy
alignment procedure may sometimes cause words to wrongly become aligned, and
prevents any many-to-one alignments. Future models may benefit from using
a soft alignment model, similar to the attention mechanisms of neural methods
in RTE \cite{bowman:2015:emnlp,parikh:2016:emnlp}. This could be accomplished
by integrating soft alignments into our lexical entailment classifier, or
by integrating our features into attention-based neural networks for RTE.

