\documentclass[letterpaper]{article}

\usepackage{times}
%\usepackage{fullpage}
\usepackage{latexsym}
\usepackage{amsmath}
%\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{aclcite}
\usepackage{alltt}
\usepackage{subfig}
\usepackage{aliases}

\title{Identifying Lexical Relationships and Subsitutes with Distributional Semantics}
\author{Stephen Roller\\
The University of Texas at Austin\\
{\tt roller@cs.utexas.edu}\\
\\
Doctoral Dissertation Proposal}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
  As the field of Natural Language Processing has developed, more ambitious
  semantic tasks are starting to be addressed, such as Question Answering, and
  Recognizing Textual Entailment. Systems which approach these tasks can
  perform sophisticated inference between sentences of Natural Language, but
  often have exhibit issues of Montague-style semantics, where the meaning of
  {\em life} is {\em life$'$}. Such systems depend heavily on lexical resources
  like WordNet to provide critical information like the relationships between
  lexical items, or whether one lexical item entails another. However, lexical
  resources are expensive to create and maintain, and can never be
  comprehensive.

  Distributional Semantics has long provided a method to automatically induce
  meaning representations for lexical items from large corpora with little or
  no annotation efforts. In Distributional Semantics, words are modeled as
  vectors in high-dimensional spaces, induced by counting or modeling the
  contexts in which a word appears. The resulting representations are excellent
  as proxies of semantic similarity: words will have similar representations if
  their semantic meanings are similar. Yet, knowing two words are similar does
  not tell us their relationship, or whether one entails the other.

  In this work, we present several techniques for automatically identifying
  specific relationships or entailments from distributional representations of
  lexical semantics. Broadly, this work falls into two distinct but related
  areas: the first predicts specific taxonomic relations and entailment
  decisions between lexical items devoid of context;
  the second predicts specific lexical paraphrases in complete sentences. In
  both cases, we evaluate and emphasize generalization to novel lexical items
  which are devoid of any explicit semantic annotations.  We also provide
  analysis and insight as to how and why these models are anble to generalize
  to novel lexical items, and relate this to prior linguistic and NLP research.

  We propose several short- and long-term extensions to this work. In the
  out-of-context models, we propose applying our model to a broader group of
  lexical relations, and including additional useful features in
  classification.  In the in-context work, we propose extensions which improve
  handling of very rare context items, analyzing the relationships between
  paraphrases. In the long-term, we propose stronger models of in-context
  representations, and unifying the out-of-context and in-context models in
  one of several possible ways.
\end{abstract}

%\pagebreak
\tableofcontents
\pagebreak

%\section*{Table of Contents}
%\begin{itemize}
%  \item Introduction
%    \begin{itemize}
%      \item Distributional Semantics
%      \item Lexical Entailment
%      \item Lexical Substitution
%    \end{itemize}
%  \item Identifying Lexical Relations
%    \begin{itemize}
%      \item Asym (COLING 2014)
%      \item TACL 2016
%    \end{itemize}
%  \item Lexical Substitution
%    \begin{itemize}
%      \item NAACL 2016
%    \end{itemize}
%  \item Proposed Work
%    \begin{itemize}
%      \item 
%    \end{itemize}
%\end{itemize}
%\pagebreak

\section{Introduction}

As the field of Natural Language Processing (NLP) has developed, significant
progress has been made in many areas of research, especially those in ``lower''
levels of the Vauquois Triangle, like Part-of-Speech tagging and Parsing.  Even
difficult semantics tasks, like Sentiment Analysis and Document Classification,
have made significant progress NEEDCITE. Today, there is great deal of emphasis
on sophisticated semantics tasks that require inference and synthesis of
knowledge. These include tasks like Question Answering (QA), where computers
must read and answer questions about passages NEEDCITE, and Recognizing Textual
Entailment (RTE), where computers must decide whether a hypothesis utterance
logically follows from a given text \cite{marelli:2014:semeval}.

Much progress has been made in systems which perform these sort of sophisticated
logical inferences, especially as common benchmarks and data sets have started
to become common \cite{marelli:2014:semeval,bowman:2015:emnlp,NEEDCITE}. Yet these
systems ultimately must work over individual lexical items to form a
conclusion, and require knowledge about the relationships between lexical
items. Consider the following example:
\footnote{Mention ``Montague's curse'', as I like to call it?}
\begin{quote}
  Text: The bright girl reads a book.\\
  Hypothesis: The smart child looks at pages of text.
\end{quote}
Any language processing system wishing to infer the second sentence from
the first must know quite a bit of world knowledge: it must know that
girl is a kind of child, and that bright and smart are synonyms in this
context; that books contain pages of text, and that reading involves looking
at some text.

Although significant progress has been made on the text of
Rich Textual Entailment, these systems ultimately depend on some lexical
resources NEEDCITE. The most famous lexical resource is WordNet, which
organizes the lexicon into a large ontology containing many thousands of
annotations of relationships between different lexical items, though countless
other resources also exist and are used (NEEDCITE FrameNet, PPDB, BLESS, LEDS,
SemEval). Unfortunately, resources as thorough as WordNet are extremely
expensive and intensive to create, and since language is ever-changing, they
are inevitably always incomplete, and represent one weak point in Natural
Language Understanding systems. Even modern Neural Network approaches,
which attempt to learn entailments without explicitly depending on these
resources, cannot make entailment predictions about words which were not
in the training data NEEDCITE.

Distributional Semantics offers a potential solution to these issues of lexical
coverage. Distributional Semantics takes inspiration from the famous quote,
``You shall know a word by the company it keeps'' \cite{firth:1957:la}. In
Distributional Semantics, representations of word meaning are automatically
induced by counting or modeling the {\em contexts} in which a word appears.
Distributional Semantics is often sometimes called Vector Space Models (VSM) of
language, as words are represented as vectors a high-dimensional vector space.
Words with similar semantics will have similar vectors in this induced
geometric space. Since VSMs do not require annotated corpora, they are used and
studied as an alternative or predictor of particular lexical resources NEEDCITE.

In this work, we approach the question as to whether Distributional Semantics
can be leveraged to predict some of the lexical inferences necessary in tasks
like RTE. Namely, we present techniques and models for predicting specific
lexical relationships, entailments, and subsitutions using Distributional
Semantics. In Lexical Relationship detection and Entailment detection, we must
predict whether two words exhibit specific relationships, like hypernymy (is-a
relationships) or meronymy (has-a relationships). We present two original
models which can learn to predict hypernymy and sometimes other relations,
and characterize their performance on different data sets, relations. We also
present an original model for Lexical Substitution, in which we must predict a
context-specific synonym for a given target word; we argue that Lexical
Substitution is a form of lexical entailment in context.

Finally, we propose several extensions to this completed work. In the
short-term, we propose drawing a stronger connection between Lexical
Substitution and lexical relationships, analyzing the sort of lexical
relationships which we can and cannot predict in our model. We propose
extending our Lexical Substitution model to better handle out-of-vocabulary
(OoV) issues for very rare context items. We also propose further analysis of
our Lexical Relationship models to see if additional relationships can be
automatically predicted, and consider whether the model can do all-words
relationship prediction.  In the long-term, we wish to consider whether our
Lexical Substitution model can be improved by removing or alleviating a strong
independence assumption in the previous model, and how the Lexical Substitution
and Relationship Detection models could be potentially unified.


\section{Background and Related Work}

\subsection{Textual Entailment}

\subsection{Distributional Semantics}
Distributional Semantics is a powerful tool for automatically inducing semantic
representations for lexical items \cite{turney:2010:jair,erk:2012:llc}.  The
core notion is that of the {\em Distributional Hypothesis}, that if two words
appear in similar {\em contexts}, they can be assumed to have similar meaning.
This idea has a long history in the semantics and philosophical literature that
can be traced back over 60 years
\cite{wittgenstein:1953:pi,harris:1954:word,firth:1957:la}. In its modern form,
Distributional Semantics involves finding {\em vector space} representations of
words which are constructed by counting or modeling the contexts in which a
particular word appears. According to the Distributional Hypothesis then, words
with similar vectors can be assumed to have similar meanings
\cite{turney:2010:jair}.

\begin{figure}
\centering
\begin{minipage}{7cm}
\begin{scriptsize}
\begin{alltt}
         the furry {dog} is friendly to
and manipulate the {dog} 's lips and
       as a clever {dog} ; two to
a reputation among {dog} trainers of having
    also among the {dog} breeds most likely
 the very earliest {dog} shows and kennel
        as a guard {dog} and to hunt
   the mechanic 's {dog} began to howl
\end{alltt}
\end{scriptsize}
\end{minipage}\qquad
\begin{minipage}{3cm}
\includegraphics[width=3cm]{figures/vsm}
\end{minipage}
\caption{Contexts of the word {\em dog}, and cartoon drawing
of word vectors.}
\label{fig:vsm}
\end{figure}

In its simplest form, vectors are induced by defining a vector space where
each dimension in the space corresponds to a particular context word. A large,
unannotated corpus of text is then iterated, finding instances of a given word,
like {\em dog}, and incrementing a count for each of the word's {\em
co-occurrences}, or words appearing to the left or right of the target word
{\em dog}, as in Figure~\ref{fig:vsm}. With a large enough corpus, coherent
statistical patterns begin to form. For example, the word {\em furry} is likely
to be used to describe both {\em cat} and {\em dog}, which is then reflected in
the vector counts. After constructing vector representations for the words {\em
cat} and {\em dog}, we can then compare these vectors using {\em cosine
similarity}:
\begin{equation*}
  \text{cosine}(u, v) = \frac{\sum_i u_iv_i}{\sqrt{\sum_i u_i^2 \sum_i v_i^2}}
\end{equation*}
Here, $i$ iterates over all the different context dimensions, like {\em furry}
or {\em kennel}, and cosine similarity is defined over the range of $[0, 1]$.
Words with similar vectors will have a small angle between them, and therefore
a high cosine similarity (e.g. close to 1).

In practice, usually the distributional vectors are more sophisticated in their
construction than raw co-occurrence counts. Typically, words and contexts below
a certain threshold are omitted from the co-occurrence matrix, as extremely
rare words have few counts and therefore impoverished representations. The
co-occurrence matrix is also usually transformed using some non-linearity;
one common choice is Positive Pointwise Mutual Information (PPMI), where the
raw co-occurence count between a word $w$ and context $c$ is transformed,
\begin{equation*}
  \text{PPMI}(w, c) = \max\left(0, \log\frac{P(w, c)}{P(w)P(c)}\right)
\end{equation*}
Pointwise Mutual Information measures roughly how many times more likely
do these two items co-occur more than chance, while Positive PMI additionally
co-occurrences which occur less often than chance. Different transformations,
like Mutual Information, conditional probability, and softplus, are also
sometimes seen in the literature, and each emphasizes different forms of
similarity.

Another important aspect of Distributional Semantics is how context is defined.
In the example of Figure~\ref{fig:vsm}, we showed that context can be defined
as three words to the left and right of the target word, but there are
alternatives. For example, using very large windows of similarity results in
emphasizing more topical similarity, e.g. doctor and hospital, while smaller
windows emphasize more functional similarity, e.g. doctor and surgeon NEEDCITE.
Context can be also defined as {\em syntactic neighbors} extracted
from a dependency parse \cite{pado:2007:cl}. For example, in Figure~\ref{fig:syn},
the contexts for the word {\em chased} would be {\em nsubj+dog} and {\em
dobj+tail}. Distributional Spaces defined in this manner tend to emphasize
the {\em selectional preferences} of words, or the tendency of certain
relations to occur with certain arguments.

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{figures/syn}
\caption{Example of a dependency parse for ``The dog chased its tail.'' In
a syntactic distributional space, the labeled graph edges define contexts
rather than a window.}
\label{fig:syn}
\end{figure}

One final notion in Distributional Semantics we wish to review is that of
dimensionality reduction. As described earlier, the distributional vector
spaces are very high-dimensional; bag-of-words spaces have many thousands of
dimensions (or sometimes as large as the entire vocab, e.g.
\newcite{pennington:2014:emnlp}, while syntactic spaces often have a million
or more dimensions, e.g. \newcite{baroni:2011:gems}. Efficiently dealing
with these large, extremely sparse vectors can sometime be troublesome, so
we often opt to use some form of {\em dimensionality reduction}, like
Singular Value Decomposition \cite{landauer:1997:pr}. In dimensionality
reduction, the co-occurrence matrix $M$ is usually assumed to be factorizable
into two lower-rank matrices,
\begin{equation*}
  M = VC^{\top}
\end{equation*}
where $V$ is some lower dimension representation of word vectors, and $C$
is the corresponding lower dimension representation of the context items.
Interestingly, the recent, popular Word2Vec algorithm \cite{mikolov:2013:iclr}
can also be viewed as a form of dimensionality reduction \cite{levy:2014:nips}.

\subsection{Lexical Entailment}

It can be difficult to give an exact definition of Lexical Entailment, but in
this document we define it as any lexical relationship where a typical,
cooperative reader would logically assume a given consequent from a given
antecedent.\footnote{Katrin, help please!  can you think of something better
with a citation I can use?} This includes many classical lexical relations,
like hypernymy (a girl {\em is a} child; a dog {\em is an} animal), and
meronomy (a girl {\em has} eyes; a dog {has a} tail), but also many
nonclassical ones too. For example, a {\em dissertation} implies a {\em
committee}, but the exact relationship between these items would be difficult
to pigeonhole into a category, or generalize to other items.

There has been a great deal of research around predicting lexical entailment
automatically from text; we cannot possibly enumerate all the work on this
problem done in all of NLP, but we aim to cover some influential approaches,
and emphasize attempts to do this with distributional semantics.

One important, early work in this task was that of Hearst patterns
\cite{hearst:1992:coling}, which are specific textual patterns highly
indicative of particular relationships. Common Hearst patterns include
phrases like ``X such as Y,'' ``X including Y,'' which are both highly
indicative of hypernymy; and possessives, like ``X's Y'', can be indicative
of meronomy. In fact, the previous sentence contains several Hearst patterns
{\em about} Hearst patterns. Later, the Hearst pattern method by
\newcite{snow:2004:nips} to include a variety of syntactic patterns rather than
pure textual patterns. In another vein of research, others have proposed how
hypernymy relationships might be viewed holistically in corpora. One
lasting hypothesis has been the Distributional Inclusion Hypothesis (DIH)
\cite{zhitomirsky-geffet:2005:acl}, which states that the contexts in which a
hypernym appears should be a superset of all its hyponyms, or subordinate
words. Though this seems to be strictly false\footnote{Consider {\em \#The animal
barked loudly at its bowl of animal food.}}, a considerable amount of work has
assumed it to be at least partially true, including our own.

Early work in identify lexical entailments using distributional spaces was
focused mostly on attempts to find unsupervised similarity measures to identify
specific relationships from the word vectors
\cite{weeds:2004:coling,clarke:2009:gems,kotlerman:2010:nle,lenci:2012:starsem,santus:2013:thesis}.
The reasoning went that, with perhaps the right corpus, the right
distributional space, and the right similarity measure, perhaps hypernym pairs,
or at least candidate pairs, could be readily identified using only word
vectors vectors. This view was further highlighted by the fact that the
ubiquitious cosine similarity tends to highlight co-hyponym pairs more than
other relations \cite{baroni:2011:gems}. Many of the proposed measures were
based on the Distributional Inclusion Hypothesis in one form or another
\cite{clarke:2009:gems}, or a hybrid of DIH and cosine similarity
\cite{kotlerman:2010:nle,lenci:2012:starsem}.

Increasingly, it became obvious that unsupervised measures did not work
as well as the community hoped, the community began working
on entailment detection as a supervised task. \newcite{baroni:2012:eacl}
proposed, as a preliminary baseline of a novel data set, training a simple
baseline classifier to predict whether word pairs were either hypernyms or
nonhypernyms. Although they reported accuracies in the high 90s, others later
realized this was due to issues of {\em lexical memorization}, or a special
kind of overfitting
\cite{roller:2014:coling,weeds:2014:coling,levy:2015:naacl}. As such, more
recent works have emphasized their performance when {\em individual words are
held out entire}, so that the same word can never appear in both training and
testing sets \cite{roller:2014:coling,kruszewski:2015:tacl,levy:2015:naacl}.
We discuss more about this issue of Lexical Memorization in
Section~\ref{sec:lexmem}.

\subsection{Lexical Substitution}

While the Lexical Entailment task described above is mostly about predicting
fixed, global relationships between words, Lexical Substitution is about
finding specific paraphrases for words {\em in a given sentential context}.
That is, we wish to also consider how polysemy can change the sort of
lexical entailments derived by a system.

Consider again our example from the introduction: while the previous section
aims to understand that a {\em girl is-a child}, it does not attempt to find a
relationship between {\em bright} and {\em smart}. This is because the word
{\em bright} has multiple meanings, and there needs to be some sort of
reasoning in order to consider that the appropriate synonym of {\em bright}
is {\em smart} in this context, but not {\em luminous} or {\em colorful}.
More formally, in Lexical Substitution, we wish to identify what
words could replace a given target word in a particular sentence, while
perserving the meaning of the whole sentence. Evaluation is typically
performed by comparing a system's predictions to actual substitutes generated
by humans asked to perform the same task.

In the original SemEval 2007 version of the Lexical Substitution Task, systems
were required to generate up to ten potential substitutes for the given target
word and context, and not given any additional information. As such, early
Lexical Substitution systems relied heavily on lexical resources like WordNet
to obtain a list of {\em candidate substitutes}. Then, using various custom
features, they would extract features and learn to rank substitutes for their
appropriateness \cite{mccarthy:2007:semeval}. Later, researchers began noting
the difference between candidate generation and candidate ranking, and then
focused research on mostly the ranking task \cite{NEEDCITE}. The vast majority
of these systems are based on the notions of finding substitutes based on {\em
Selectional Restrictions} and {\em Selectional Preferences}, or
limitations/preferences about the types of arguments and modifiers words tend
to take \cite{NEEDCITE}.

Today, many have begun returning to the generation-and-ranking task
\cite{kawakami:2015:arxiv,melamud:2015:naacl,roller:2016:naacl}, but opting to
avoid lexical resources like WordNet.  Curiously, to date, the best performing
systems in Lexical Substitution are ones which are unsupervised with respect to
the task: either they are trained for some auxiliary task like Machine
Translation \cite{kawakami:2015:arxiv}, or they are similarity methods over
distributional models \cite{roller:2016:naacl} or language modeling
\cite{melamud:2015:naacl}. This could be because of a lack of recent effort in
directly supervised versions of the task \cite{szarvas:2013:naacl}, or because
lexical substitution data sets are relatively small compared to the size of the
corpora used in machine translation and distributional semantics.

\section{Completed work}

\subsection{Asym Model for Lexical Entailment \cite{roller:2014:coling}}

In 2014, we presented a novel, supervised model for the task of Lexical
Entailment Detection. At the time, the primary
motivation for the model was to develop a classifier that was inherently
asymmetric: baseline similarity scores like cosine have the unfortunate
property that
\begin{equation*}
  \text{cosine}(a, b) = \text{cosine}(b, a).
\end{equation*}
However, lexical relationships are very often asymmetric: although {\em girl}
implies {\em child}, the the opposite is does not hold. As such, trying to
perform lexical entailment based solely on symmetric measures will always
fail hard.

To remedy this, we proposed Asym, a supervised classifier based on the
{\em vector difference} between two word vectors. The asym model provided
rough conditional probability that two words $a$ and $c$ exhibited a particular
lexical relation (the antecedent and consequent). The model learned a simple
Logistic Regression classifier with the vector difference and the vector
difference element-wise-squared as input features:
\begin{equation*}
  \text{asym({a}, {c})} = \sigma\left({w}^\top \langle{a} - {c}; ({a} - {c})^2\rangle + b\right)
\end{equation*}
The model was parameterized by a weight vector $w$ and intercept $b$, which are
learned from training data.

One significant advantage of this model was its direct connection to the
Distributional Inclusion Hypothesis: since our model used the vector difference
as input, it was naturally measuring whether $a_i$ was greater than $c_i$,
effectively acting as a strict-subset measurement. The element-wise-squared
part of the input features capture whether they have a large absolute difference,
allowing asym to capture the ``equal'' part of the ``less than or equal''
relation. As such, the model describes a form of {\em Selective Distributional
Inclusion Hypothesis}, which presupposes that the DIH holds, but only for
particularly relevant dimensions.

To evaluate our model, we trained and measured accuracy of the asym model on
two data sets in a variation of leave-one-out cross validation (LOOCV) and
measuring absolute accuracy. In this variation of LOOCV, we selected one word
from the vocabulary in the data sets, and considered {\em all pairs} with that
word to be test pairs. The remainder of word pairs, which did not contain the
held out word, were treated as training pairs. This prevented classifiers from
memorizing that words like {\em animal} are simply more likely to be hypernyms.
All models were trained on a 

We evaluate our model on two data sets. The first, {\bf LEDS}
\cite{baroni:2012:eacl}, contains 1385 hyponym-hypernym pairs as positive
examples and 1385 negative pairs which were generated by randomly shuffling the
positive examples. As such the model only contains hypernymy and random
relations, and we trained a binary classifier.  The second data set is {\bf
BLESS} \cite{baroni:2011:gems}, which contains annotations  of word relations
for 200 unambiguous, concrete nouns from 17 broad categories. Each noun is
annotated with its co-hyponyms, meronyms, hypernym and some random words. Since
there were four relations, we trained four one-vs-all classifiers, and
predicted the relation with the highest score; in this way, the model
can actually learn to detect 3 types of lexical relations, though this
was not our primary research interest at the time.

Table~{\ref:asymresults} shows our initial experimental results for the
Asym classifier.

\begin{table}
  \begin{center}
  \begin{tabular}{|l|cc|cc|}
  \hline
  Data set        &  \multicolumn{2}{c|}{\bless } & \multicolumn{2}{c|}{\entailment} \\
  \hline
  Random Baseline        &  \multicolumn{2}{c|}{.46 } &  \multicolumn{2}{c|}{.50 } \\
  \hline
  Classifier                 &   \svm          &   {\logregs}                             &   \svm          &   {\logregs}          \\
  \hline
  \UpWWr           &    .76          & {\bf.84}       &     .81         & {\bf.85}         \\
  \UpSr            &    .73          &     .80            &     .78         &     .82            \\
  \typedmr         &     -           &     .82            &     .65         &  {\bf.85}            \\
% \mikolovr        &    .46          &     .77            \\
  \hline
  \end{tabular}
  \end{center}
  \caption{Average accuracy of {\svm} and {\logreg}
on {\bless} and {\entailment}
  using different spaces for feature generation.}
  \label{tab:bless-and-entailment-v-space}
\end{table}



\begin{table}
\begin{center}
\begin{tabular}{|l|rrrr|}
  \hline
  Model                          &   LEDS   &  BLESS   &   Medical &      TM14  \\
  \hline
  Cosine (Baseline)              &    .789  &    .225  &    .165  &      .674  \\
  \hline
  \newcite{baroni:2012:eacl}     &          &          &          &            \\
  Concat                         &    .802  &    .689  &    .257  &      .722  \\
  Diff                           &    .809  &    .496  &    .226  &      .672  \\
  Asym \cite{roller:2014:coling} &    .878  &    .546  &    .227  &      .677  \\
  Ksim \cite{levy:2015:naacl}    &{\bf.912} &    .596  &{   .278} &  {\bf.730} \\
  HD   \cite{roller:2016:arxiv}  &    .909  &{\bf.712} &{\bf.295} &      .726  \\
  \hline
\end{tabular}
\end{center}
\caption{Mean F1 scores for each model and data set.}
\label{tab:results1}
\end{table}



\begin{table}
  \centering
  \begin{tabular}{|l|cc cc|}
    \hline
    Measure        &\small \coord     &\small \hyper    &\small \mero      &\small \randomn  \\
    \hline\hline
    \multicolumn{5}{|c|}{DIH Measures Before Dimension Selection}\\
    \hline
    \cosine        &     .68     &     .20    &     .27     &     .27    \\
%    \balAPinc      &     .56     &     .23    &     .31     &     .28    \\
%    \WeedsPrec     &     .52     &     .22    &     .33     &     .28    \\
    \ClarkeDE      &     .66     &     .19    &     .28     &     .28    \\
    \invCL         &     .60     &     .18    &     .31     &     .28    \\
    \hline
    \hline
    %Measure        &\small \coord     &\small \hyper    &\small \mero      &\small \randomn  \\
    %\hline\hline
    % ------------------------------------------------------------------
    % THESE VALUES WERE CREATED IN THE HOLD-ONE-CONCEPT-OUT SETTING
    % ------------------------------------------------------------------
    \multicolumn{5}{|c|}{DIH Measures After Dimension Selection}\\
    \hline
    \cosine        &   .69      &    .20    &    .24     &    .28    \\
%    \balAPinc      &   .54      &    .35    &    .26     &    .28    \\
%    \WeedsPrec     &   .50      &    .38    &    .27     &    .29    \\
    \ClarkeDE      &   .55      &    .39    &    .24     &    .29    \\
    \invCL         &   .42      &{\bf.58}   &    .24     &    .29    \\
    % ------------------------------------------------------------------
     \hline
  \end{tabular}
  \caption{Mean Average Precision for the unsupervised measures before
  after selecting the top dimensions from the Asym model.}
  \label{tab:mapscores}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{plots/window}
  \caption{Caption here.}
  \label{fig:windowsize}
\end{figure}

%\begin{figure}
%  \centering
%  \includegraphics[width=0.75\textwidth]{plots/kernel}
%  \caption{Caption here. Note this graph should probably be redone given the tuning procedure.}
%  \label{fig:kernel}
%\end{figure}




In order to test how well our supervised model is capturing the notion
of selective distributional inclusion, we test each of the unsupervised
measures on a smaller space, limited only to the dimensions preferred by
the classifier. We emphasize that we do {\em not} aim to show that our
supervised method outperforms unsupervised methods, but rather
that the unsupervised methods benefit greatly from feature selection.
Additionally, we analyze  which dimensions are selected by
the classifier to facilitate understanding of why these dimensions are
important.

We train the {\logreg} classifier using the dimensionality-reduced {\UpWWr} space with the
same method we use in Section~\ref{sec:supervised}.
% For each concept in {\bless}, we hold out one concept at a time and
% train the {\logreg} classifier on the other 199 concepts using the {\UpWWr}
% space. Again, we also remove training pairs which share a relata with
% the training set. 
We take the classifier's learned
hyperplane separating hypernyms from other relations, and project
the hyperplane back into the original {\UpWW} space.\footnote{Ideally
  we would train on the original space to
  inspect the relevant dimensions. However, there are more dimensions than examples,
  so we train on the SVD space and backproject.}
We select the 500 dimensions in the original space that are most
relevant according to the classifier weights, and
test the unsupervised measures on this new space, which we denote as
{\UpWWu}.\footnote{
Note that
{\UpWWu} varies slightly from concept to concept, since the hyperplane is
learned on a per-concept basis.
It is important that we use the linear {\logreg} classifier for this
reverse-projection procedure, as the separating hyperplane {\em must} be linear
in order to complete the projection. In particular, the hyperplane in the
{\svm} classifier cannot be easily backprojected, since it exists in a higher
dimensional space than the projection matrix.
Furthermore, it is
important that we use a classifier trained using the difference features because of
its analogy to the Distributional Inclusion Hypothesis.}

The 500  most relevant dimensions are selected as follows:
We select the 250 most negatively weighted original
dimensions using the difference features $f$. 
These are the features that have smaller values for hyponyms
(e.g.\ \textit{dog}) than for hypernyms  (e.g.\
\textit{animal}), so they characterize hypernymy.
We further select the 250 most positively weighted
original dimensions using the squared-differences features $g$. These
are the ones where a large difference does not indicate hypernymy. 

Table~\ref{tab:mapscores:unprojected} shows the MAP scores for three
of the 
measures in the new {\UpWWu} space. (The results for {\balAPinc} and
{\WeedsPrec} are slightly worse than {\ClarkeDE}.)
%We see the same overall picture as the
%boxplots in Figure~\ref{fig:unprojectedplots}.  
All measures except for {\cosine}
assign higher scores to hypernyms than they did in the original space
(compare to {\UpWW} part of Table~\ref{tab:mapscores}).
But it is only {\invCL} that ranks hypernyms significantly higher 
than co-hyponyms.\footnote{Wilcoxon signed-rank test, $p < .001$. 
% \gbt{I don't think we need significance testing here, since it's
% such a clear result, but I'm leaving it here commented out just in
% case y'all want to include it}
% The {\invCL} performs extremely well in the unprojected space, significantly
% outperforming all other measures at distinguishing hypernymy, and significantly
% outperforming {\invCL} measures computed using the {\UpWW} space.
To check that the measures are being improved by the dimension selection and not just by restricting to a smaller space,
%As a control, 
we evaluated the similarity measures on a variation
of the {\UpWW} space which uses 500 randomly selected dimensions from the
original space.  The results are approximately unchanged from those on the original
{\UpWW} space.
}
%% This indicates that the measures are not being improved
%% just by restricting to a smaller space, but rather by the dimension
%% selection.

For this experiment, we train on all of {\bless}
except for one concept and then evaluate the unsupervised models on
the held-out concept -- that is a setting that could, in principle, be
used as a hypernymy detector. If we instead train the supervised model
on all of {\bless} to determine an upper bound of how well dimension
selection can do on this dataset, MAP for {\invCL} rises to .67.



\subsection{Subsystem in complete RTE system}

Table~\ref{tab:evallexical} shows performance of the classifier on {\em only}
the lexical rules, which have single words on the LHS and RHS. In these
experiments we use the same procedure as before, but omit the phrasal rules
from the dataset. On the RTE tasks, we compute accuracy over only the SICK
pairs which require at least one lexical rule. Note that a new ceiling
score is needed, as some rules require both lexical and phrasal
predictions, but we do not predict any phrasal rules.

\begin{table}
\centering
\begin{tabular}{|lrrr|}
    \hline
    {\bf Feature set} & {\bf Intrinsic} & {\bf RTE Train} & {\bf RTE Test}\\
    \hline
    Always guess neutral & 56.6 & 69.4 & 69.3 \\
    Gold standard annotations&100.0 & 93.2 & 94.6 \\
    \hline
    Wordform only        & 57.4 & 70.4 & 70.9 \\
    WordNet only         & 79.1 & 83.1 & 84.2 \\
    Dist (Lexical) only  & 68.8 & 76.3 & 76.7 \\
    \newcite{roller:2014:coling} only            & 76.8 & 78.3 & 79.2 \\
    \hline
    All features         & 84.6 & 82.7 & 83.8 \\
    \hline
\end{tabular}
\caption{Cross-validation accuracy on Entailment on lexical rules only}
\label{tab:evallexical}
\end{table}

Again we see that WordNet features have the highest contribution. Distributional rules still perform better
than the baseline, but the gap between distributional features and
WordNet is much more apparent. 
Perhaps most encouraging is the very high performance of the Asymmetric features: by
themselves, they perform substantially better
than just the distributional features. We investigate this further
below in Section~\ref{subsubsec:asym}. 

As with the entire dataset, we once again see that all the features are
highly complementary, and intrinsic accuracy is greatly improved by
using all the features together. It may be surprising that these significant
gains in intrinsic accuracy do not translate to improvements on the
RTE tasks; in fact, there is a minor drop from using all features compared
to only using WordNet. This most likely depends on {\em which} pairs the
system gets right or wrong. For sentences involving multiple lexical rules,
errors become disproportionately costly. As such, the high-precision
WordNet predictions are slightly better on the RTE task.

In a qualitative analysis comparing a
classifier with only cosine distributional features to a classifier
with the full feature set, we found that, as expected, the distributional features
miss many hypernyms and falsely classify many co-hyponyms as
entailing: We manually analyzed a sample of 170 pairs that the distributional classifier
falsely classifies as entailing. Of these, 67 were co-hyponyms (39\%), 33 were
antonyms (19\%), and 32 were context-specific pairs like
\textit{stir/fry}. On the other hand, most (87\%)
 cases of entailment that the distributional classifier detects but the
all-features classifier misses are word pairs that have no link in
WordNet. These pairs include \textit{note
  $\to$ paper}, \textit{swimmer $\to$ racer}, \textit{eat $\to$ bite},
and \textit{stand $\to$ wait}. 

\subsection{H-Features and Multi-prototype Hypernymy Classifier}
\label{sec:multiprototype}


\subsubsection{Concerning Lexical Memorization}
\label{sec:lexmem}

Partially motivation for methodology of 

\begin{table}
\begin{center}
  \begin{small}
  \begin{tabular}{|llll|}
    \hline
    LEDS & BLESS & Medical & TM14\\
    \hline
     %% baroni           %% bless             %% levy               %% turney_lemma           \\
     material       &      goods             &     item           &      sensitiveness          \\
     structure      &      lifeform          &     unlockable     &      tactility              \\
     object         & {\bf item}             &     succor         &      palate                 \\
     process        & {\bf equipment}        &     team-up        &      stiffness              \\
     activity       & {\bf herbivore}        &     non-essential  &      content                \\
%     environment    &      omnivore         %&     40px           &      ductility              \\
%     element        & {\bf creature}        %&     crimefighter   &      musculature            \\
%{\bf practice}      &      life-form        %&     material       &      dimensionality         \\
%     nature         & {\bf artifact}        %&     assitance      &      quality                \\
%     organism       & {\bf pest}            %&     nourishment    &      angulation             \\
    \hline
  \end{tabular}
  \end{small}
\end{center}
\caption{Most similar words to the prototype $\hat H$ learned by the Concat model. Bold items
appear in the data set.}
\label{tab:wordsim}
\end{table}



\begin{table}
\begin{center}
  \begin{small}
  \begin{tabular}{|ll|}
    \hline
    LEDS & BLESS\\
    \hline
      nmod:such\_as+animal             &  nmod:such\_as+submarine           \\
      acl:relcl+identifiable           &  nmod:such\_as+ship                \\
      nmod:of\depinv+determine         &  nmod:such\_as+seal                \\
      nmod:of\depinv+categorisation    &  nmod:such\_as+plane               \\
      compound+many                    &  nmod:such\_as+rack                \\
      %nmod:such\_as+pot                &  nmod:such\_as+rope                \\
      %nmod:such\_as+bone               &  nmod:such\_as+box                 \\
      %nmod:of\depinv+strangeness       &  nmod:such\_as+bat                 \\
      %amod+unassociated                &  nmod:such\_as+pot                 \\
      %compound+similar                 &  nmod:such\_as+container           \\
    \hline
    Medical & TM14\\
    \hline
      nmod:such\_as+patch              &  amod+desire                       \\
      nmod:such\_as+skin               &  amod+heighten                     \\
      nmod:including+skin              &  nsubj\depinv+disparate            \\
      nmod:such\_as+tooth              &  nmod:such\_as+honey               \\
      nmod:such\_as+feather            &  nmod:with\depinv+body             \\
      %nmod:including+finger            &  nsubj\depinv+unconstrained        \\
      %nmod:such\_as+ear                &  compound\depinv+gratification     \\
      %nmod:such\_as+heart              &  compound\depinv+comfort     \\
      %nmod:such\_as+foot               &  nsubj\depinv+composite            \\
      %compound+similar                 &  nmod:such\_as+label               \\
\hline
  \end{tabular}
  \end{small}
\end{center}
\caption{Most similar contexts to the prototype $\hat H$ learned by the Concat model.}
\label{tab:ctxsim}
\end{table}


Knowing that the Concat classifier acts primarily as a feature detector, we
ask whether this can be combined with similarity-based insights of models like
Ksim and Cosine. To this end, we propose a novel model which exploits the
Concat classifier, extends its modeling power, and adds two other types of
evidence proposed in the literature: overall similarity, and distributional
inclusion.


The model works through an iterative procedure similar to Principal Component
Analysis (PCA). Each iteration repeatedly trains a Concat classifier under the
assumption that it acts as a feature detector, and then explicitly discards
this information from the distributional vectors. By training a new feature
detector on these modified distributional vectors, we can find additional
features indicative of entailment which were not captured by the first
classifier. This is similar to how in Principal Component Analysis, the
second principal component is computed after the first principal component
has been removed from the data.

The main insight is that after training some feature detector using Concat,
we can {\em remove} this feature from the distributional vectors through
the use of {\em vector projection}.
Formally, the vector projection of $x$ onto
a vector $\hat p$, $\text{proj}_{\hat p}(x)$ finds the {\em component} of $x$
which is in the direction of $\hat p$,
\begin{equation*}
  \text{proj}_{\hat p}(x) = \left(\frac{x^\top\hat p}{\|\hat p\|}\right)\hat p.
\end{equation*}
Figure~\ref{fig:vecproj} gives a geometric illustration of the vector
projection. If $x$ forms the hypotenuse of a right
triangle, $\text{proj}_{\hat p}(x)$ forms a leg of the triangle. This also
gives rise to the {\em vector rejection}, which is the vector forming the third
leg of the triangle. The vector rejection is orthogonal to the projection, and
intuitively, is the original vector after the projection has been removed:
\begin{equation*}
  \text{rej}_{\hat p}(x) = x - \text{proj}_{\hat p}(x).
\end{equation*}

\begin{figure}
  \begin{center}
  \includegraphics[width=0.30\textwidth]{vecproj}
\end{center}
\caption{A vector $\hat p$ is used to break $H$ into two orthogonal components,
its projection and the rejection over $\hat p$.}
\label{fig:vecproj}
\end{figure}

Using the vector rejection, we take a learned Hearst Pattern detector $\hat p$,
and remove these features from each of the data points. That is, for every data
point $\langle H, w\rangle$, we replace it by its vector rejection and rescale
it to unit magnitude:
\begin{align*}
  H' & = \text{rej}_{\hat p}(H) / \|\text{rej}_{\hat p}(H)\|\\
  w' & = \text{rej}_{\hat p}(w) / \|\text{rej}_{\hat p}(w)\|
\end{align*}
A new classifier trained on the $\langle H', w'\rangle$ data must now learn
a very different decision plane than $\hat p$, as $\hat p$ is no longer present
in any data points. This new classifier must perform strictly worse than the
original, otherwise the first classifier would have learned this hyperplane.
Nonetheless, it will be able to learn {\em new} Hearst patterns which the
original classifier was unable to capture. By repeating this process several
times, we can find several Hearst pattern detectors, $\hat p_1, \ldots, \hat
p_n$.

In each iteration $i$ of the procedure, we generate a four-valued feature vector
$F_i$, based on the Hearst pattern detector $\hat p_i$. Each
feature vector contains (1) the similarity of $H_i$ and $w_i$ (before projection);
(2) the Hearst detector
$\hat p_i$ applied to $H_i$; (3) the Hearst detector $\hat p_i$ applied to $w_i$; and
(4) the difference of 2 and 3.
\begin{align*}
  & F_i(\langle H_i, w_i\rangle, \hat p_i)\\
  & \qquad = \langle H_i^\topw, H_i^\top\hat p_i, w_i^\top\hat p_i, H_i^\top\hat p_i - w_i^\top\hat p_i\rangle
\end{align*}
These four ``meta''-features capture all the benefits of the Hearst pattern
detector (slots 2 and 3), while still addressing Concat's issues with
similarity arguments (slot 1) {\em and} distributional inclusion (slot 4).

The union of all the feature vectors $F_1, \ldots, F_n$ from repeated iteration form a
$4n$-dimensional feature vector which we use as input to another classifier.
This classifier is trained on the exact same training data as each of the
individual Hearst Pattern detectors, so the procedure only acts as a method of
feature extraction. We use an SVM with an RBF-kernel, as we found it to work
best, though several nonlinear classifiers also to do well.

The only free hyperparameter of the model is $n$, the number of iterations of
the PCA-like procedure. We found that performance improves substantially on all
data sets with at least two iterations, but then falls off slowly after five or
more iterations. We chose to use four iterations for all final models.

\subsection{Lexical Substitution}

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{figures/substitution}
  \caption{Caption here.}
  \label{fig:substitution}
\end{figure}



We propose a new measure, called Probability-in-Context (PIC), based
on SGNS context vectors to estimate the appropriateness
of a lexical substitute. Similar to \balAddCos, the measure has two equally-weighted,
independent components measuring the appropriateness of the substitute
for both the target and the context, each taking the form of a softmax:\footnote{Note that $P(s|t)$ measures paradigmatic similarity
  of $s$ and $t$, while $P(s|C)$ is syntagmatic fit to the
  context.
  %Both take the same shape of a dot product.
  For $P(s|t)$,
  Mikolov et al.~\shortcite{mikolov:iclr13} show that cosine
  similarity of SGNS embeddings predicts
  paradigmatic similarity. $P(s|C)$ can be interpreted as the PMI of
  $s$ and $C$~\cite{Levy:2014tp}.}
\begin{align*}
  \mbox{PIC}(s | t, C) &= P(s | t) \times P(s | C)\\
  P(s | t) &= \frac{1}{Z_t}\exp\left\{s^\top t\right\}\\ %{\sum_{s'}\exp\left\{s'\cdot t\right\}}
  P(s | C) &= \frac{1}{Z_C}\exp\left\{\sum_{c\in C}s^\top\left[Wc + b\right]\right\}
\end{align*}

The values $Z_t$ and $Z_C$ are normalizing constants to make sure each
distribution sums to one. This measure has two free parameters, $W$ and $b$,
which act as a linear transformation over the context vectors. These parameters
are estimated from the {\em original corpus}, and are trained to maximize
the prediction of a {\em target} from only its syntactic contexts (c.f. Section~\ref{sec:training}).
Given this formulation, a natural question is why not train the embeddings to optimize the
softmax directly? We choose to parameterize the measure rather than the
embeddings because (i) SGNS embeddings are already popular and readily
available and
(ii) it ensures the quality of embeddings remains constant across experimental 
settings.

To measure the importance of parameterization, we
also compare to a non-parameterized PIC (\ourmeas), which only uses a softmax over the
dot product:
\begin{align*}
  \mbox{nPIC}(s | t, C) &= P(s | t) \times P_n(s | C)\\
  P_n(s | C) &= \frac{1}{Z_n}\exp\left\{\sum_{c\in C}s^\top c\right\} %{\sum_{s'}\exp\left\{\sum_{c\in C}{s'\cdot c}\right\}}
\end{align*}



\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|}
  \hline
  {\bf Measure} & {\bf SE07} & {\bf Coinco} & {\bf TWSI2}\\
  \hline\hline
  \multicolumn{4}{|c|}{Candidate Ranking (GAP)}\\
  \hline
  \ooc               &     44.2   &     44.5  &     57.9       \\
  \addCos            &     51.2   &     46.3  &     62.2       \\
  \balAddCos         &     49.6   &     46.5  &     61.3       \\
  \hline
  \ourmeas           &     51.3   &     46.4  &     61.8       \\
  \ourmeasparam      & {\bf52.4}  & {\bf48.3} & {\bf62.8}      \\
  \hline\hline
  \multicolumn{4}{|c|}{All-Words Ranking (Mean Precision@1)}\\
  \hline
  \ooc               &     11.7   &    10.9   &      9.8       \\
  \addCos            &     12.9   &    10.5   &      7.9       \\
  \balAddCos         &     13.4   &    11.8   &      9.8       \\
  \hline
  \ourmeas           &     17.3   &    16.3   &     11.1       \\
  \ourmeasparam      & {\bf19.7}  &{\bf18.2}  & {\bf13.7}      \\
  \hline
  \hline
  \multicolumn{4}{|c|}{All-Words Ranking (Mean Precision@3)}\\
  \hline
  \ooc               &     9.7    &     8.6   &     7.0       \\
  \addCos            &     9.0    &     7.9   &     6.1       \\
  \balAddCos         &     9.8    &     9.1   &     7.4       \\
  \hline
  \ourmeas           &    13.1    &    12.1   &     7.9       \\
  \ourmeasparam      &{\bf14.8}   &{\bf13.8}  &{\bf10.1}      \\
  \hline
\end{tabular}
\end{center}
\caption{Lexical Substitution results for candidate ranking (GAP) and all-words
ranking tasks (P@1, P@3).}
\label{tab:precision}
\end{table}

Table~\ref{tab:precision} contains results for all measures across all
experimental settings.

The first observation we make is that the \ourmeasparam~measure performs best
in all evaluations on all data sets by a significant margin.\footnote{Wilcoxon signed-rank test, $p < 0.01$}
In the GAP evaluation, all measures
perform substantially better than the \ooc~baseline, and the \ourmeas~measure
performs comparably to \balAddCos. We note that context-sensitive
measures give the most improvement in SE07, reflecting its greater emphasis on
polysemy.

As we turn to the all-words ranking evaluations, we observe that the absolute numbers are
much lower, reflecting the increased difficulty of the task. We also see the
that \ourmeas~and \ourmeasparam~both improve greatly over all baselines: The
\ourmeas~measure is a relative 30\% improvement over \balAddCos~in SE07 and
Coinco, and the \ourmeasparam~measure is a relative 50\% improvement over
\balAddCos~in 5 evaluations.

Since both measures have a clear improvement over the baselines, especially in
the more difficult all-words task, we next strive to understand why.

\begin{table*}[t]
  \begin{center}
  \begin{tabular}{|cccc|}
    \hline
    \ooc             & \balAddCos            & \ourmeas         & \ourmeasparam\\
    \hline\hline
    \multicolumn{4}{|c|}{You can sort of challenge them well, did you}\\
    \multicolumn{4}{|c|}{{\bf really} know the time when you said yes?}\\
    \hline
    {    trully              } & {    proably             } & {    realy               } & {\bf actually            } \\
    {\bf actually            } & {    trully              } & {\bf truly               } & {\bf truly               } \\
    {    actaully            } & {    acutally            } & {\bf actually            } & {    already             } \\
    {    acutally            } & {    actaully            } & {    hardly              } & {    barely              } \\
    {    proably             } & {    probaly             } & {\bf definitely          } & {    just                } \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Example where the \ourmeasparam~performs better in the All-Words Ranking task. The target word and correct answers
  are bolded.}
  \label{tab:cherry}
\end{table*}

We first provide a cherry-picked examples to give intuitions about why
our model performs better. Table~\ref{tab:cherry} contains the cherry example,
where our model performs better than prior work. While OOC
and \balAddCos~both suggest replacements with reasonable semantics, but are
all misspelled. \ourmeas~and \ourmeasparam~only pick words with the correct spellings,
with the exception of ``realy.''

We consider a few experiments with this hypothesis that the measures do better
because they capture better {\em unigram} statistics than the baselines. Recent literature found that the vector
norm of SGNS embeddings correlates strongly with word frequency
\cite{wilson:arxiv15}. We verified this for ourselves, computing the Spearman's
rank correlation between the corpus unigram frequency and the vector length
and found $rho = 0.90$, indicating the two correlate very strongly. Since the
dot product is also the unnormalized cosine, it follows that \ourmeas~and
\ourmeasparam~should depend on unigram frequency.

To verify that the \ourmeas~and \ourmeasparam~measures are indeed preferring more frequent
substitutes, we compare the single best predictions (P@1) of the \balAddCos~and
\ourmeas~systems on all-words prediction on Coinco. Roughly 42\% of the predictions made
by the systems are identical, but of the remaining items, 74\% of
predictions made by \ourmeas~have a higher corpus frequency than \balAddCos~(where
chance is 50\%). We find \balAddCos~and \ourmeasparam~
make the same prediction 37\% of the time, and \ourmeasparam~predicts a
more frequent word in 83\% of remaining items. The results for SE07 and
TWSI2 are similar.

This indicates that the unigram bias is even higher for \ourmeasparam~than
\ourmeas. To gain more insight, we manually inspect the learned parameters $W$
and $b$. We find that the $W$ matrix is nearly diagonal, with the values along
the diagonal normally distributed around $\mu=1.11$ ($\sigma=0.02$) and the rest
of the matrix normally distributed roughly around 0 ($\mu$=2e-5, $\sigma$=0.02). This
is to say, the \ourmeasparam~model is approximately learning to {\em exaggerate} the
magnitude of the dot product, $s^\top c$. This suggests one could even replace our
parameter $W$ with a single scaling parameter, though we leave this
for future work.

To inspect the bias $b$, we compute the inner product of the $b$ vector with
the word embedding matrix, to find each word's a priori bias, and correlate it with word frequencies. We find
$rho=0.25$, indicating that $b$ is also capturing unigram
statistics.

Is it helpful in lexical substitution to prefer more frequent substitutes? To
test this, we pool all annotator responses for all contexts in Coinco, and find
the number of times a substitute is given correlates strongly with frequency
($rho=0.54$).

These results emphasize the importance of incorporating unigram
frequencies when attempting the lexical substitution task (as with
many other tasks in NLP). Compared to
cosine, the dot product in \ourmeas~stresses unigram frequency, and
the parameters $W$ and $b$ strengthen this tendency.
%  (as with so many
% other tasks in NLP). Such a result may seem obvious in retrospect, but many
% prior works in Lexical Substitution simply do not take this into account.

\pagebreak
\section{Proposed Work}

We now describe the general proposed methods of further research. The proposed
future work breaks into two broad categories: short-term proposals,
which must be completed for the final thesis, and long-term proposals, which
are more ambitious research directions that may take much longer to successfully
pursue.

The proposed short term work focuses predominantly on how the more recent
successful research may contribute to a larger RTE system: While the completed
work has shown successful results on the two tasks of Lexical Entailment and
Lexical Substitution, these newer models have not yet been applied to the end
goal of Textual Entailment. We propose multiple methods to test our models
in an end-to-end RTE system.

The long term work follows from three broad directions forward: (1) encouraging
taxonomic consistency of predictions in Lexical Entailment; (2) better integration
of a wider context into the Lexical Substitution system; and (3) a more sophisticated
distributional model which reuses information when possible.

\subsection{Short Term Proposals}

In this section, we propose and discuss several additional experiments which
should be completed for the final thesis. We break the section into experiments
for Lexical Entailment, and experiments for Lexical Substitution. All are
proposed primarily with integration into a final RTE subsystem in mind.

\subsubsection{Lexical Entailment}

\paragraph{H-features for non-Hypernymy Relations}

In Section~\ref{sec:multiprototype}, we discussed how the multi-prototype model
is able to exploit multiple H-feature classifiers in order to improve the results
and modeling power of a hypernymy detection system. Indeed the multi-prototype
model outperformed the Asym and Ksim models when evaluated on the same metric of
F1-score in hypernymy prediction.

However, there exist many relations {\em other} than hypernymy are useful to
distinguish in textual entailment: for example, we saw in
Section~\ref{sec:rtesubsystem} that cosine similarity is indicative of entailment
at moderate levels, where it reflects general similarity, but {\em counter-indicative}
of entailment at very high levels, where it is most indicative of co-hyponymy.
Indeed, dividing word relationships into hypernymy and non-hypernymy could be
viewed as dividing the world into living and nonliving things: while this is
a very important distinguishing feature, it may be helpful to identify a bit more
information.

Indeed, the results shown in Table~\ref{tab:bless-and-entailment-v-space} show
the accuracy of the Asym and Baroni classifiers on a four-way relationship
prediction task: hypernymy, cohyponymy, meronomy, and random, yet the
experiments in Section~\ref{sec:multiprototype} only describe performance in a
binary hypernymy-or-not classification task. We propose the multi-prototype
model of Section~\ref{sec:multiprototype} be extended and evaluated on its
performance in other relationships.

There are multiple ways that the model could be extended. The one we believe
will be most successful is one that trains several binary multi-prototype models:
one for hypernymy-vs-nonhypernymy, one for meronomy-vs-nonhypernymy, etc.
Similar to how the PCA procedure was used only as a form of feature-extraction
for the final prediction, each of the binary classifier iterations will be also
used for feature extraction for a final classifier.
That is, we will use the procedure described in Section~\ref{tab:multiprototype}
to extract several iterations of features for hypernymy, then completely repeat
the procedure for meronomy and co-hyponymy. The resulting features from each
of the classifiers will be concatenated for a final four-way classifier prediction.
Another alternative would be to try to learn the four-way classifiers concurrently
(e.g., a softmax instead of logistic regression), and extract the corresponding
H-features at this level.

There are interesting research questions that stem from this procedure, beyond
just final performance scores. One is what distributional-level features will
be learned as prototypical of meronyms, or co-hyponyms? As we saw in
Section~\ref{sec:prototypicality}, the classifier automatically learned to pick
out well-known Hearst patterns, indicative of hypernymy. It remains to be seen
whether it will pick out additional Hearst patterns indicative of other
relations: for example, simple conjunctions for co-hyponymy (e.g., {\em cats
and dogs}) or the possessive for meronomy (e.g., {\em cat's tail}).

\paragraph{Multiprototypes and H-features for RTE}

The results of Section~\ref{sec:rtesubsystem} showed that the Asym hypernymy
prediction model can provide strong improvements in a complete, end-to-end RTE
system, especially when combined with traditionally hand-engineered features.
Yet, as we also saw in Section~\ref{sec:multiprototype}, the multiprototype
model allows for greater modeling power and significant improvements on multiple
lexical entailment systems. Yet we do not yet know how much the multiprototype
model is able to contribute as one part of the larger RTE pipeline.

We suspect, given its substantial improvements over Asym in lexical data sets,
that it will also be able to be able to improve the full system. However, it
remamins unclear what is the best way to integrate it with additional
hand-engineered features used in the RTE system.  One possibility would be to
simply use the multiprototype's feature extraction procedure to provide
additional information to the lexical entailment classifier. Yet the classifier
used in Section~\ref{sec:rtesubsystem} was a linear classifier over the hand
engineered features, which outperformed nonlinear versions. Unfortunately, we
found in Section~\ref{sec:multiprototype} that the final ensemble classifier
{\em must} be nonlinear, else it does not gain any additional power in the
repeated iterations. Resolving this conflict may be difficult, though we
suspect it may be alleviated by more careful hyperparameter tuning of nonlinear
models.

These proposed experiments will also need to be tightly coupled with the
complete non-hypernymy detector discussed in the previous section. If H-features
are useful useful for predicting co-hyponymy or meronomy, it also stands to
reason that they should be useful in the final RTE subsystem. We will need to
explore the best form of combination with both kept in mind.

\subsubsection{Lexical Substitution in RTE}

In Section~\ref{sec:tasks}, we argued that Lexical Substitution is strongly
related to the tasks of lexical and textual entailment, partially standing
as a form of context-specific synonym detection. Unfortunately, we have
yet to provide any results indicating that Lexical Substitution may actually
contribute to the complete RTE system.

We propose that the Lexical Substitution model described in
Section~\ref{sec:lexsubmodel} be used to provide additional information to
the Lexical Entailment classifier used in Section~\ref{sec:rtesubsystem}.

There are at least two possibilities for how a Lexical Substitution model could
contribute to the task. The first, and one we believe most likely to be
helpful, is to simply add the context-similarity features of the Lexical
Substitution model as another feature in the Lexical Entailment classifier.
That is, for a pair of words in the SICK data set, measure the similarity of
their contexts using the $P(s|C)$ or $P_n(s|C)$ values proposed in
Equation~\ref{eqn:pic} and Equation~\ref{eqn:npic}. This could possibly help
with some tricky cases where two senses of a word could lead to different conclusions.
For example, ``EXAMPLE HERE.''

It may also be necessary to use a similar trick to the cosine binning technique
described in Section~\ref{sec:rtesubsystem}: it could be that different levels
of contextual similarity give different indications as to entailment, especially
given the close relationship between our Lexical Substitution model and
cosine similarity. Variations on this trick may also be necessary to test:
since our Lexical Substitution model gives a score over the entire vocabulary,
the binning may need to be logarithmic, or measured as a percent of vocabulary
which is a better substitute than the queried consequent.

The second possibility for how Lexical Substitution could contribute is in
the {\em alignment} preprocessing procedure: while we currently only choose
which word pairs to align with our greedy cosine-similarity based method, it
could be that the $P(s|C)$ model provides additional information to make
smarter alignments. For example, we each token in the antecedent, we could
align it with the consequent's token that acts as the best lexical substitute
{\em in-context}, rather than the out-of-context measure we presently use.

Ultimately though, we suspect improvements to RTE from either proposed
application of the Lexical Substitution model may be marginal, or even
negative, as polysemy is not common in the RTE data set that we use. We may
need to manually identify which RTE pairs exhibit polysemy the strongest, or
manually construct a small data set of sentences with polysemy in order to
show positive results.

\subsection{Long Term Proposals}

We now describe some possible longer term research questions which could be
addressed in the final thesis. Success in any of these items would
be significant contributions to the field, and are therefore more ambitious and
risky than the short-term proposals.

\subsubsection{Taxonomic Consistency in Hypernymy Prediction}

Presently in our hypernymy-prediction models, relationships between all
pairs of words are made independently: the prediction of whether ``cat is an
animal'' has no bearing on whether ``dog is an animal'', aside from their
distributional similarity. While this is a straightforward application of
machine learning principles to the problem, it ignores one important fact: that
hypernymy is just one aspect of a complete, well-structured ontology or
taxonomy. Yet, since we predict each word pair individually, there is no
guarantee the final output of the system over all pairs will also be
well-structured.

For example, a hypernymy prediction model could predict that both ``animal is a
hypernym of dog'' and that ``dog is a hypernym of animal'', is though we know
hypernymy is non-reflexive; or it could predict that ``a golden retriever is a
dog'' and ``dog is an animal'' but incorrectly predict that ``gold retriever is
{\em not} an animal,'' violating the transitive property of hypernymy. These
properties of hypernymy are inherent to how it is defined, and our models
should be take this into account.

With this in mind, is it possible to modify our model such that taxonomic
consistency of the output is guaranteed, or at least more consistent? One
possibility would be to use the {\em confidence scores} associated with the
hypernymy predictions, and simply revise the least-confident predictions to
be consistent in a post-processing step. For example, in our reflexive example
above, the more confident of the two predictions will be assumed to be
the correct one.

This idea would likely even benefit further from our short-term proposal to
see how well the multi-prototype model does at predicting relations other
than hypernymy: for example, if we are highly confident that two words are
co-hyponyms, then we can become more confident that they share the same
hypernyms, and vice versa.

This idea of enforcing taxonomic consistency is not new: it was previously
explored by \newcite{caraballo:1999:acl}, who use a clustering algorithm in
order to find co-hyponym terms, and then predict their hypernym using entire
clusters. It was also examined some in \newcite{snow:2004:nips}, who used
syntactic Hearst Patterns to make predictions about hypernymy, and then
linearly interpolated the probabilities of predictions in order to better
conform to these hard taxonomic constraints. Later, \newcite{snow:2006:acl}
proposed a probabilistic model over {\em taxonomies}, allowing them to propose
an algorithm for searching over entire taxonomies constrained by rules about
the transitivity of hypernymy, and reflexivity of co-hyponymy. This taxonomy
search is then used in order to find the single taxonomy which has maximal
probability according to the evidence provided by Hearst patterns, while also
not violating any of the hard constraints.

Although \newcite{snow:2006:acl} found their taxonomy searching algorithm
highly successful with the use of the lexico-syntactic patterns indicative
of hypernymy, such an approach is yet to be tried on classifiers which make
use of distributional information about words. Therefore, the most reasonable
first course of action would to reimplement and examine whether their model
is compatible with the distributional models of hypernymy prediction. On one
hand, it could help tremendously; on the other hand, their model supports only
two forms of constraints (transitivity of hypernymy and reflexivity of
co-hyponyms), leaving open questions about how other constraints should be
imposed.

Another possibility is through the use of the same technology powering the
end-to-end RTE system of \newcite{beltagy:2016:cl}: Markov Logic Networks
(MLNs).  Markov Logic Networks provide a framework for doing probabilistic
logical inference over sets of weighted atoms and logical rules. MLNs are given
a set of weighted First Order Logic (FOL) rules and a database of atoms, and
give reweighted predictions about the probabilities of atoms based on their
consistency with the logical rules. MLNs can encode many
different relations as rules, and perform joint updating of all lexical
relationship predictions.  For example, the rules for transitivity and reflexivity
discussed in \newcite{snow:2006:acl} could be encoded as:
\begin{align*}
  \forall x,y,z. &~\text{hyper}(x,y) \wedge \text{hyper}(y, z) \rightarrow \text{hyper}(x, z),\\
  \forall x,y. &~\text{cohyp}(x,y) \leftrightarrow \text{cohyp}(y,x),
\end{align*}
but other rules, such that co-hyponyms share a hypernym, may also be encoded:
\begin{align*}
  \forall x,y,z. &~\text{cohyp}(x,y) \wedge \text{hyper}(x,z) \rightarrow \text{hyper}(y,z).
\end{align*}

MLNs ability to incorporate {\em weighted rules} would also give room for
flexibility in constraint importance, and allow for {\em some} violations of
constraints when the evidence is simply overwhelming. Therefore, we believe
both the model of \newcite{snow:2006:acl} and an MLN-based model to be strong
candidates for improving lexical relationship prediction by enforcing taxonomic
consistency.

\subsubsection{Sophisticated Contexts for Lexical Substitution}

\paragraph{Wider Contexts in Lexical Substitution}
In the model of Lexical Substitution discussed in Section~\ref{sec:pic}, we
showed how the syntactic neighbors of a target word are useful in predicting
what are the lexical substitutes, or in-context synonyms, of the target word.
Although the syntactic neighbors can indeed capture some kinds of {\em long-distance
dependencies}, there is much greater deal of context available which is
presently not used by the model: namely, the {\em entire rest of the sentence}.

Consider, for example, if the model were able to asked to find the best verb
to fill-in-the-blank in the following two simple sentences:

\begin{center}
  {\em The dog \_\_\_ the tennis ball.}\\
  {\em The dog \_\_\_ the meat ball.}
\end{center}

In both cases, the model will be given the exact same context for the missing
verb: its subject should be {\em dog} and its object should be {\em ball}. However,
humans know that the dog is more likely to {\em chase} or {\em fetch} the tennis
ball, while it is more likely to {\em eat} the meat ball. Without being provided
the additional information about the ball, the model has absolutely no way
to distinguish these two cases. How can this information be integrated into the
model?

Even beyond this simple example, it's already clear from prior work that
additional context can be very useful in the task.
\newcite{vandecruys:2011:emnlp}, for example, proposed model which combined
a syntactic distributional space with a bag-of-words distributional space with
a very large window by finding a shared latent representation of both spaces,
showing modest improvements over each individual space. Furthermore,
\newcite{kawakami:2016:iclr} obtained state-of-the-art results on Lexical
Substitution using a neural network language model which encodes the {\em
entire sentence} as input, though their model also depends heavily on the use
of machine translation data as a sort of proxy for Word Sense Disambiguation
information. Nonetheless, it is clear that there is more useful information
available than our own Lexical Substitution model is allotted.

There two distinct ways we could implement wider contexts into our Lexical
Substitution model: using linguistic knowledge, and using neural networks.
In the former, we could simply use existing linguistic knowledge in order to
model additional syntactic patterns from large corpora. That is, in
addition to modeling the direct neighbors of words which constructing our
distributional space, we could also add pattern-based rules for modeling
indirect neighbors. For example, we could introduce an additional context
in our distributional space for {\tt dobj+compound+\_+meat}; that is, an
additional column for which the direct object of a verb is compounded with
``meat'', and so on for other nouns. Indeed, since our model uses collapsed
prepositional phrases, this is already partially implemented (e.g., we already
model ``to the store'' as {\tt prep:to\_store} rather than just {\tt prep:to}).

A variation of this approach was discussed in \newcite{pado:2007:cl}, the
original proposal of syntactic distributional spaces. In their model, they also
extracted contexts for dependency chains two and three hops away from the
target word, rather than the only the direct neighbors. Since then, most models
have focused mostly on direct neighbors (except for prepositional phrases), as
this approach introduces many additional contexts, making the model
significantly more complex and memory intensive, while also dramatically
increasing the sparsity of the data: the vast majority of 3-hop syntactic
patterns will occur only once in corpus. Indeed, if we use this linguistic
approach, these issues of scaling and sparsity will plague our model as well.

The other possibility for this problem would be to employ modern neural network
models, like the Long Short Term Memory (LSTM) model \cite{hochreiter:1997:nc}.
The LSTM model has become extremely popular in the field of Natural Language
Processing, thanks to recent advances in network initialization \cite{glorot},
training techniques \cite{adam,adagrad}, and hardware like Graphics Processing
Units (GPUs). Indeed, LSTMs are the basis for the successful model of
\newcite{kawakami:2016:iclr} mentioned previously. LSTMs are a form of a
Recurrent Neural Network, which takes as input a variable-length sequence of
items (tokens), and make some prediction. They have been successfully applied
countless areas of NLP, including language modeling \cite{needcite}, sentiment
analysis \cite{needcite}, machine translation \cite{needcite}, textual entailment
\cite{needcite}, question answering \cite{needcite}, and recently even
lexical entailment \cite{schwartz:2016:acl}.

More concretely, we could integrate wider syntax using an approach similar to
that of \newcite{schwartz:2016:acl}. In their model, a syntactic chains
connecting two target words was used to classify whether the words stand in a
hypernymy relationship or not, acting as a modern neural-network version of
\newcite{snow:2004:nips}. We propose using similar syntactic chains to simply
predict the {\em last word} in the chain, performing the operation for billions
of syntactic chains extracted from large corpora. This model would have the
capability of capturing and remembering relevant long-distance information
whenever it is helpful in predicting the final word. The greatest challenge
in applying this model is scaling it, as LSTM models can take days or weeks
to train when their final output is a prediction over the entire vocabulary.

\paragraph{Joint Models of Syntactic Neighbors}

Another issue with the lexical substitution model discussed in Section~\ref{sec:pic}
is that the model fundamentally assumes independence between each of the syntactic
neighbors. Ultimately, the model suggests substitutes in the above examples by
asking, ``What does a dog do? What is done to a ball? What is the intersection
of these two sets?'' That it uses the unnormalized inner products in Equation~\ref{eqn:pic}
means that one of these questions may be given more weight than another, but they
are still considered independently: the question is never ``What does a dog do
to a ball?''

It is worth considering whether this independence assumption can be relaxed
in some way. Intuitively, it seems obvious that a joint model should be helpful.
Yet, one major issue is that the number of syntactic neighbors is variable:
consider that a verb may have attachments may have only one attachment (intransitive),
two attachments (transitive), or more (ditransitive, prepositional phrase attachments,
adverbs, etc). Similarly, nouns can also stand in many syntactic relations
simultaneously (compound, adjective, prepositional phrase, verb relation, and
others). Since the number of attachments is variable, it's difficult to define
a coherent probabilistic model which does not demand at least {\em some}
independence assumptions. Even if we were to define the model over all possible
syntactic relations for every word, the issue would not be solved: consider
a ``small, furry, brown mouse,'' which has three modifiers all standing
in the {\tt adjmod} relation.

Even if we could define such a model, it would likely be plagued the typical
extreme sparsity issues: for many sentences in our data set, an {\em exact}
combination of attachments seen is unlikely to appear anywhere, even in
extremely large corpora. Therefore, a naive joint model is likely to estimate
the probability as zero for most everything.

As with the proposed methods for including wider contexts, we could potentially
address these issues using either linguistic knowledge, or neural networks.  To
address the concerns using linguistic knowledge, we could again mark certain
rules should be modeled jointly, and assume independence between the remaining
syntactic relations. For example, we could model transitive verbs jointly, but
assume independence from adverbs and prepositional modifiers; or we could
jointly model two adjective modifiers, but assume independence once we
encounter three or more. Again, rules similar to the ones discussed in
\newcite{pado:2007:cl} would be a good starting point, and others could be
proposed in qualitative analysis of the data.

The other possibility is to use LSTMs in order to jointly model the different
modifiers. Since LSTMs are able to handle a variable-length sequence as input,
they seem to be a good candidate for creating a joint model over all the
attachments in a principled manner. Unfortunately, LSTMs also assume that the
{\em order} of the sequence is important, which is not the case for our problem:
there is no inherent ordering of syntactic attachments. We could canonicalize
the ordering (simply insisting that, for example, subjects are always presented
before objects), but it remains unclear whether this is useful. We could also
{\em randomize} the order the attachments are presented: since there are many
permutations, this could substantially increase the amount of training data
available, since each sentence could then generate many training examples: one
where subject is first, one where object is first, etc. However, preliminary
experiments showed difficulty with this second approach: the $PIC$ model came
about when our first attempts at using LSTMs were unsuccessful.

Nonetheless, it is encouraging that the same linguistic and neural network
approaches could be potentially useful for both introducing wider context,
and joint modeling.

\subsubsection{Generalized Distributional Spaces for Entailment and Substitution}

Our final long term proposal is fairly different from the other two: while
those were ideas to directly impact performance on the two tasks we have
since focused our attention, our last idea focuses more broadly on how
we can improve distributional spaces altogether.

We begin with an observation about the process described in
Section~\ref{sec:dist} when constructing a syntactic distributional space.
When constructing a distributional space, the final step involves finding a
matrix factorization for the extremely large, extremely sparse word-context
co-occurrence matrix. In syntactic spaces, the contexts are distinguished by
the neighboring word {\em together} with its syntactic relation. That is there
is one context for {\tt nsubj\_dog} and another for {\tt dobj\_dog}. This is a
powerful notion, and one that gives the syntactic spaces their ability
to model selectional preferences: without distinguishing these relations,
the model would be much like to the bag-of-words spaces.

Yet, modeling each of these contexts independently also seems to be wasting
a large amount of information: that it is the same lexical item being modified
by two different syntactic relations. If this information is ignored, then
we must assume we have excellent corpus statistics for how every word stands
in every relation. Yet this rarely happens, and one of the important
steps in constructing a syntactic distributional space is to limit modeling
to only the top one million most frequent contexts, where the statistics are
still plentiful enough for modeling. But in defining a cutoff at all, we
are also ignoring a lot of information: by Zipf's law, half of the data
is in the long tail. Do we really want to throw away all this data?

We ask then, to what degree can the {\em syntactic relations} be separated
from the {\em words} in syntactic distributional spaces? 
Can these context vectors we rely heavily on in our work be {\em generatively
modeled}? Perhaps one way to approach this would be to model it as a
composition function:
\begin{equation*}
  \vec{ {\tt nsubj\_dog} } = f({\tt nsubj}, \vec{dog})
\end{equation*}
In this view, we would need to learn one function, $f$, which took in a syntactic
relation and a dog, and produced a {\em context vector} on how dog acts in the
subject position. One possibility would be to use vector concatenation,
treating the relation and the word as separate components, independent of each
other. Intuitively, this does not seem to capture the the essence of our
proposal, but it could act as a baseline model:
\begin{equation*}
  \vec{ {\tt nsubj\_dog} } = \langle \vec{dog} ; \vec{nsubj} \rangle
\end{equation*}

Another possibility would be to treat the procedure as a form of post-processing
generalization. For example, we could learn the context vectors as we do now, and
then attempt to learn to predict the observed context vectors from the input
components. For example, perhaps after performing the concatenation, a simple
linear regression could combine them:
\begin{equation*}
  \vec{ {\tt nsubj\_dog} } = W\langle \vec{dog} ; \vec{nsubj} \rangle + \vec{b}
\end{equation*}
This is a relatively weak model, but it would still enable us to predict
information about novel combinations of syntactic relations and words. The
model could be even more sophisticated, being a neural network instead; however,
neural networks are known to be easier to train for classification than
regression \cite{needcite}.

Another variation of this linear regression idea, would be to model each
syntactic relation as a function {\em applied} to the vector. For example,
we would learn a separate $W$ for {\tt nsubj}, {\tt dobj}, etc:
\begin{equation*}
  \vec{ {\tt nsubj\_dog} } = W_{{\tt nsubj}}\vec{dog}
\end{equation*}
This gives the model many more free parameters, but may present some difficulty
for the rarest syntactic relations, where there could easily be more parameters
in $W$ than examples, though perhaps heavy regularization of $W$, or even
constraining it to be diagonal could be beneficial. Modeling the behavior in
this way would draw greater parallels to other areas of the literature,
like compositional distributional semantics \cite{needcite}, the
bilinear models proposed in GloVe \cite{pennington:2014:acl}, and the recursive
composition model proposed by \newcite{needcite}.

Alternatively, we could  try to learn the distributional vectors from scratch
using a similar model. Recall that the dimensionality reduction procedure
simply tries to predict the Pointwise Mutual Information between a word and
a context. Rather than performing the dimensionality reduction, then regression,
we could encode our generative principals into the dimensionality reduction.
In this way, a successful model may look something more like {\em tensor
factorization}. For example, we could model the pmi between a word ($v$),
and a relation-context pair ($r, c$) as:
\begin{equation*}
  PMI(v, r, c) = {\bf v}^\top {\bf W}_{r} {\bf c},
\end{equation*}
where ${\bf W_r}$ is a matrix. This is essentially the same as the linear
regression model proposed in the above paragraph, but performed at a different
step. Unfortunately, tensor factorization can be very difficult
\cite{needcite}, and there is no such convenient canonical factorization for
tensors, as the SVD is for matrices \cite{needcite}. Finding the ideal
factorization could be a very difficult optimization problem \cite{needcite}.

Regardless, if we assume that we {\em can} find any model which successfully
predicts context vectors in a generative manner, then it could lead to
substantial improvements on both tasks of Lexical Entailment {\em and} Lexical
Substitution. For example, we saw in Table~\ref{tab:ctxsim}

\section{Conclusion}

\pagebreak
\bibliographystyle{aclcite}
\bibliography{refs}

\end{document}
