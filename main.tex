\documentclass[letterpaper]{article}

\usepackage{times}
%\usepackage{fullpage}
\usepackage{latexsym}
\usepackage{amsmath}
%\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{aclcite}
\usepackage{alltt}
\usepackage{subfig}

\title{Identifying Lexical Relationships and Subsitutes with Distributional Semantics}
\author{Stephen Roller\\
The University of Texas at Austin\\
{\tt roller@cs.utexas.edu}\\
\\
Doctoral Dissertation Proposal}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
  As the field of Natural Language Processing has developed, more ambitious
  semantic tasks are starting to be addressed, such as Question Answering, and
  Recognizing Textual Entailment. Systems which approach these tasks can
  perform sophisticated inference between sentences of Natural Language, but
  often have exhibit issues of Montague-style semantics, where the meaning of
  {\em life} is {\em life$'$}. Such systems depend heavily on lexical resources
  like WordNet to provide critical information like the relationships between
  lexical items, or whether one lexical item entails another. However, lexical
  resources are expensive to create and maintain, and can never be
  comprehensive.

  Distributional Semantics has long provided a method to automatically induce
  meaning representations for lexical items from large corpora with little or
  no annotation efforts. In Distributional Semantics, words are modeled as
  vectors in high-dimensional spaces, induced by counting or modeling the
  contexts in which a word appears. The resulting representations are excellent
  as proxies of semantic similarity: words will have similar representations if
  their semantic meanings are similar. Yet, knowing two words are similar does
  not tell us their relationship, or whether one entails the other.

  In this work, we present several techniques for automatically identifying
  specific relationships or entailments from distributional representations of
  lexical semantics. Broadly, this work falls into two distinct but related
  areas: the first predicts specific taxonomic relations and entailment
  decisions between lexical items devoid of context;
  the second predicts specific lexical paraphrases in complete sentences. In
  both cases, we evaluate and emphasize generalization to novel lexical items
  which are devoid of any explicit semantic annotations.  We also provide
  analysis and insight as to how and why these models are anble to generalize
  to novel lexical items, and relate this to prior linguistic and NLP research.

  We propose several short- and long-term extensions to this work. In the
  out-of-context models, we propose applying our model to a broader group of
  lexical relations, and including additional useful features in
  classification.  In the in-context work, we propose extensions which improve
  handling of very rare context items, analyzing the relationships between
  paraphrases. In the long-term, we propose stronger models of in-context
  representations, and unifying the out-of-context and in-context models in
  various ways.

\end{abstract}

\pagebreak
\section*{Table of Contents}
\begin{itemize}
  \item Introduction
    \begin{itemize}
      \item Distributional Semantics
      \item Lexical Entailment
      \item Lexical Substitution
    \end{itemize}
  \item Identifying Lexical Relations
    \begin{itemize}
      \item Asym (COLING 2014)
      \item TACL 2016
    \end{itemize}
  \item Lexical Substitution
    \begin{itemize}
      \item NAACL 2016
    \end{itemize}
  \item Proposed Work
    \begin{itemize}
      \item 
    \end{itemize}
\end{itemize}
\pagebreak

\section{Introduction}

As the field of Natural Language Processing (NLP) has developed, significant
progress has been made in many areas of research, especially those in ``lower''
levels of the Vauquois Triangle, like Part-of-Speech tagging and Parsing.  Even
difficult semantics tasks, like Sentiment Analysis and Document Classification,
have made significant progress NEEDCITE. Today, there is great deal of emphasis
on sophisticated semantics tasks that require inference and synthesis of
knowledge. These include tasks like Question Answering (QA), where computers
must read and answer questions about passages NEEDCITE, and Recognizing Textual
Entailment (RTE), where computers must decide whether a hypothesis utterance
logically follows from a given text \cite{marelli:2014:semeval}.

Much progress has been made in systems which perform these sort of sophisticated
logical inferences, especially as common benchmarks and data sets have started
to become common \cite{marelli:2014:semeval,bowman:2015:emnlp,needcite}. Yet these
systems ultimately must work over individual lexical items to form a
conclusion, and require knowledge about the relationships between lexical
items. Consider the following example:
\footnote{Mention ``Montague's curse'', as I like to call it?}
\begin{quote}
  Text: The bright girl reads a book.\\
  Hypothesis: The smart child looks at pages of text.
\end{quote}
Any language processing system wishing to infer the second sentence from
the first must know quite a bit of world knowledge: it must know that
girl is a kind of child, and that bright and smart are synonyms in this
context; that books contain pages of text, and that reading involves looking
at some text.

Although significant progress has been made on the text of
Rich Textual Entailment, these systems ultimately depend on some lexical
resources NEEDCITE. The most famous lexical resource is WordNet, which
organizes the lexicon into a large ontology containing many thousands of
annotations of relationships between different lexical items, though countless
other resources also exist and are used (NEEDCITE FrameNet, PPDB, BLESS, LEDS,
SemEval). Unfortunately, resources as thorough as WordNet are extremely
expensive and intensive to create, and since language is ever-changing, they
are inevitably always incomplete, and represent one weak point in Natural
Language Understanding systems. Even modern Neural Network approaches,
which attempt to learn entailments without explicitly depending on these
resources, cannot make entailment predictions about words which were not
in the training data NEEDCITE.

Distributional Semantics offers a potential solution to these issues of lexical
coverage. Distributional Semantics takes inspiration from the famous quote,
``You shall know a word by the company it keeps'' \cite{firth:1957:la}. In
Distributional Semantics, representations of word meaning are automatically
induced by counting or modeling the {\em contexts} in which a word appears.
Distributional Semantics is often sometimes called Vector Space Models (VSM) of
language, as words are represented as vectors a high-dimensional vector space.
Words with similar semantics will have similar vectors in this induced
geometric space. Since VSMs do not require annotated corpora, they are used and
studied as an alternative or predictor of particular lexical resources NEEDCITE.

In this work, we approach the question as to whether Distributional Semantics
can be leveraged to predict some of the lexical inferences necessary in tasks
like RTE. Namely, we present techniques and models for predicting specific
lexical relationships, entailments, and subsitutions using Distributional
Semantics. In Lexical Relationship detection and Entailment detection, we must
predict whether two words exhibit specific relationships, like hypernymy (is-a
relationships) or meronymy (has-a relationships). We present two original
models which can learn to predict hypernymy and sometimes other relations,
and characterize their performance on different data sets, relations. We also
present an original model for Lexical Substitution, in which we must predict a
context-specific synonym for a given target word; we argue that Lexical
Substitution is a form of lexical entailment in context.

Finally, we propose several extensions to this completed work. In the
short-term, we propose drawing a stronger connection between Lexical
Substitution and lexical relationships, analyzing the sort of lexical
relationships which we can and cannot predict in our model. We propose
extending our Lexical Substitution model to better handle out-of-vocabulary
(OoV) issues for very rare context items. We also propose further analysis of
our Lexical Relationship models to see if additional relationships can be
automatically predicted, and consider whether the model can do all-words
relationship prediction.  In the long-term, we wish to consider whether our
Lexical Substitution model can be improved by removing or alleviating a strong
independence assumption in the previous model, and how the Lexical Substitution
and Relationship Detection models could be potentially unified.

\section{Background and Related Work}

\subsection{Distributional Semantics}
\begin{itemize}
  \item What is distributional semantics, picture, what is it good for
  \item How are vectors computed (bow, pmi, dim reduction)
  \item Dependency vectors, and the importance of col representations
  \item Important citations: Katrin's survey, Peter's survey
\end{itemize}

Distributional Semantics is a powerful tool for automatically inducing semantic
representations for lexical items \cite{turney:2010:jair,erk:2012:llc}.  The
core notion is that of the {\em Distributional Hypothesis}, that if two words
appear in similar {\em contexts}, they can be assumed to have similar meaning.
This idea has a long history in the semantics and philosophical literature that
can be traced back over 60 years
\cite{wittgenstein:1953:pi,harris:1954:word,firth:1957:la}. In its modern form,
Distributional Semantics involves finding {\em vector space} representations of
words which are constructed by counting or modeling the contexts in which a
particular word appears. According to the Distributional Hypothesis then, words
with similar vectors can be assumed to have similar meanings
\cite{turney:2010:jair}.

\begin{figure}
\centering
\begin{minipage}{7cm}
\begin{scriptsize}
\begin{alltt}
         the furry {dog} is friendly to
and manipulate the {dog} 's lips and
       as a clever {dog} ; two to
a reputation among {dog} trainers of having
    also among the {dog} breeds most likely
 the very earliest {dog} shows and kennel
        as a guard {dog} and to hunt
   the mechanic 's {dog} began to howl
\end{alltt}
\end{scriptsize}
\end{minipage}\qquad
\begin{minipage}{3cm}
\includegraphics[width=3cm]{figures/vectors}
\end{minipage}
\caption{Contexts of the word {\em dog}, and cartoon drawing
of word vectors.}
\label{fig:vsm}
\end{figure}

In its simplest form, vectors are induced by defining a vector space where
each dimension in the space corresponds to a particular context word. A large,
unannotated corpus of text is then iterated, finding instances of a given word,
like {\em dog}, and incrementing a count for each of the word's {\em
co-occurrences}, or words appearing to the left or right of the target word
{\em dog}, as in Figure~\ref{fig:vsm}. With a large enough corpus, coherent
statistical patterns begin to form. For example, the word {\em furry} is likely
to be used to describe both {\em cat} and {\em dog}, which is then reflected in
the vector counts. After constructing vector representations for the words {\em
cat} and {\em dog}, we can then compare these vectors using {\em cosine
similarity}:
\begin{equation*}
  \text{cosine}(u, v) = \frac{\sum_i u_iv_i}{\sqrt{\sum_i u_i^2 \sum_i v_i^2}}
\end{equation*}
Here, $i$ iterates over all the different context dimensions, like {\em furry}
or {\em kennel}, and cosine similarity is defined over the range of $[0, 1]$.
Words with similar vectors will have a small angle between them, and therefore
a high cosine similarity (e.g. close to 1).

\subsection{Lexical Entailment}
\begin{itemize}
  \item What is Lexical Entailment, how is it defined?
  \item Examples and types of Entailment
    \begin{itemize}
      \item Word relationships (countless kinds)
    \end{itemize}
  \item Linguistic theories, indicators of entailment
  \begin{itemize}
    \item Distributional Inclusion Hypothesis
    \item Hearst Patterns
  \end{itemize}
  \item Works predicting lex entailment using distributional semantics
    \begin{itemize}
      \item Far too extensive to thoroughly cover all groups and models attempting
      to do lexical entailment. Some use vision, some use DS, some explicitly mine
      Wikipedia for phrases, etc
    \end{itemize}
\end{itemize}

\subsection{Lexical Substitution}
\begin{itemize}
  \item What is lexical substitution
  \item Sometimes even more difficult to define that lexical entailment
  \item Linguistic understanding of substitution is less developed
    \begin{itemize}
      \item Violations of selectional preferences?
      \item Synonyms/Entailment in context?
    \end{itemize}
\end{itemize}

\section{Completed work}

\subsection{Hypernymy and Entailment Detection}

\subsubsection{Asym}

\subsubsection{}


\pagebreak
\bibliographystyle{aclcite}
\bibliography{refs}

\end{document}
