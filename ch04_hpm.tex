\chapter{Improvements in Hypernymy Detection and the HPM Model}

Chapter overview

\section{H-Features for Hypernymy Classification (Roller and Erk, 2016b)}
\label{sec:hfeatures}

In the previous sections, we saw that the Asym classifier is able to reasonably
learn to classify word pairs as hypernymy and non-hypernymy, and that is able
to contribute in an end-to-end RTE system. However, we also saw in our
RTE experiments that Asym can be improved upon by simply concatenating the Asym
difference vectors with vectors for the LHS and the RHS (which we call Concat).
In this section, we discuss some of the strengths and weaknesses of the Concat
model, and how these relate to the Asym model. We then propose a novel
classification model which combines and extends the strengths of all these
models using an iterative procedure similar to Principal Component Analysis
(PCA).

Nonetheless, this raises a concern about what the difference terms $H - w$
actually provide. We propose a qualitative experiment which explains, in
clear terms, why these terms are valuable, and leads to another model to
extend this behavior. For simplicity, we focus our analysis on the linear
Concat classifier, which exhibits the same behavior as Diff, but in a
more obvious way.

In our qualitative experiment, we train a linear Concat classifier using
syntactic distributional vectors on four separate data sets. We then analyze
the trained models by comparing their hyperplanes to the {\em context vectors}.
That is, we explicitly compare the $\hat H$ vector to the syntactic context
matrix $C$ in Equation~\ref{eqn:factorization}. This is a radically
different view of than the prototypicality hypothesis of
\newcite{levy:2015:naacl}: rather than learning a prototype of hypernymy, our
interpretation is that the Concat and Diff models learn to act as {\em feature
detectors}, which identifies features (i.e. syntactic contexts), which are
useful in identifying hypernymy.  This interpretation and corresponding
experiment is a one of our core contributions to the literature.

We train the model on four data sets: LEDS, BLESS, Medical, and TM14. LEDS and BLESS
were also used in the Asym experiments, and are datasets covering hypernymy and
non-hypernymy relations. Medical is a dataset of pairs of medical words and
entailment labels, and was farmed using Information Extraction techniques
\cite{levy:2014:conll}. Finally, TM14 contains many varied word relations (like
cause-effect, agent-object) which are annotated with entailment decisions by
\newcite{turney:2015:nle}.

\begin{table}
\begin{center}
  \begin{small}
  \begin{tabular}{|ll|}
    \hline
    {\bf LEDS} & {\bf BLESS}\\
    \hline
    \ctx{nmod:such\_as+animal}            &  \ctx{nmod:such\_as+submarine} \\
    \ctx{acl:relcl+identifiable}          &  \ctx{nmod:such\_as+ship}      \\
    \ctx{nmod:of\depinv+determine}        &  \ctx{nmod:such\_as+seal}      \\
    \ctx{nmod:of\depinv+categorisation}   &  \ctx{nmod:such\_as+plane}     \\
    \ctx{compound+many}                   &  \ctx{nmod:such\_as+rack}      \\
    \hline\hline
    {\bf Medical} & {\bf TM14}\\
    \hline
    \ctx{nmod:such\_as+patch}             &  \ctx{amod+desire}             \\
    \ctx{nmod:such\_as+skin}              &  \ctx{amod+heighten}           \\
    \ctx{nmod:including+skin}             &  \ctx{nsubj\depinv+disparate}  \\
    \ctx{nmod:such\_as+tooth}             &  \ctx{nmod:such\_as+honey}     \\
    \ctx{nmod:such\_as+feather}           &  \ctx{nmod:with\depinv+body}   \\
    \hline
  \end{tabular}
  \end{small}
\end{center}
\caption{Most similar contexts to the $\hat H$ hyperplane learned by a Concat classifier.}
\label{tab:ctxsim}
\end{table}

Table~\ref{tab:ctxsim} shows the five contexts most similar to the hyperplane
learned from each of the four datasets, and immediately explains why these models
perform strongly.  Nearly all of the contexts preferred by the model take the
form of Hearst patterns \cite{hearst:1992:coling,snow:2004:nips}.  The most
recognizable and common pattern learned is the ``such as'' pattern, as in
``animals such as cats''.  These patterns have been well known to be indicative
of hypernymy for over two decades. Other interesting patterns are the
``including'' pattern (``animals including cats'') and ``many'' pattern (``many
animals''). Although we list only the five most similar context items for the
data sets, we find similar Hearst Pattern type contexts continue to dominate
the list for the next 30-50 items.

Altogether, it is remarkable that the model identified these patterns using
{\em only} distributional vectors and only the positive/negative example pairs.
Since the model can be interpreted as a sort of {\em feature detector}, we
call this model the H-feature Detector Model.
We now show how these H-features
can be improved using an iterative procedure similar to Principal Component
Analysis.

\subsection{The H-Feature Detector Model}

Knowing that the Concat classifier acts primarily as a feature detector, we ask
whether this can be combined with similarity-based insights of models like
Asym. To this end, we propose a novel model which exploits the H-feature
Detector model, extends its modeling power, and also adds in features for
general similarity and distributional inclusion.

The model works through an iterative procedure similar to Principal Component
Analysis (PCA). Each iteration repeatedly trains a Concat classifier under the
assumption that it acts as a feature detector, and then explicitly {\em discards}
this information from the distributional vectors. By training a new feature
detector on these modified distributional vectors, we can find additional
features indicative of entailment which were not captured by the first
classifier. This is similar to how in Principal Component Analysis, the
second principal component is computed after the first principal component
has been accounted for.

The main insight is that after training some feature detector using Concat,
we can {\em remove} this feature from the distributional vectors through
the use of {\em vector projection}.
Formally, the vector projection of $x$ onto
a vector $\hat p$, $\text{proj}_{\hat p}(x)$ finds the {\em component} of $x$
which is in the direction of $\hat p$,
\begin{equation*}
  \text{proj}_{\hat p}(x) = \left(\frac{x^\top\hat p}{\|\hat p\|}\right)\hat p.
\end{equation*}
Figure~\ref{fig:vecproj} gives a geometric illustration of the vector
projection. If $x$ forms the hypotenuse of a right
triangle, $\text{proj}_{\hat p}(x)$ forms a leg of the triangle. This also
gives rise to the {\em vector rejection}, which is the vector forming the third
leg of the triangle. The vector rejection is orthogonal to the projection, and
intuitively is ``leftover'' vector after the projection has been removed:
\begin{equation*}
  \text{rej}_{\hat p}(x) = x - \text{proj}_{\hat p}(x).
\end{equation*}

\begin{figure}
\centering
\begin{tikzpicture}[scale=1.5]
    % Draw axes
    \def\r{0.5}
    % Draw two intersecting lines
    \coordinate (origin) at (0,0);
    \coordinate (x) at (45:3.0);
    \coordinate (proj) at (15:2.59807621);
    \coordinate (p) at ($(proj) + (15:1.0)$);

    \coordinate (rightangle) at ($(proj) + (150:0.25)$);
    \coordinate (toproj) at ($(proj) - (75:0.35355339059)$);
    \coordinate (torej) at ($(proj) + (45:0.35355339059)$);
    \coordinate (onproj) at (intersection of toproj--rightangle and origin--proj);
    \coordinate (onrej) at (intersection of torej--rightangle and proj--x);
    \draw [-,Gray] (onproj) -- (rightangle) -- (onrej);

    \draw [->,Gray,thick] (proj) -- (p) node[below,pos=0.5] {$\hat p$};
    \draw [->,Black,ultra thick] (origin) -- (x) node[above,pos=0.5] {$x$};
    \draw [->,Orange,very thick] (proj) -- (x) node[right,pos=0.5] {rej$_{\hat p}x$};
    \draw [->,Cerulean,very thick] (origin) -- (proj) node[below,pos=0.5] {proj$_{\hat p}x$};
\end{tikzpicture}
\caption{A vector $\hat p$ is used to break $H$ into two orthogonal components,
its projection and the rejection over $\hat p$.}
\label{fig:vecproj}
\end{figure}

Using the vector rejection, we take a learned H-feature detector $\hat p$,
and remove these features from each of the data points. That is, for every data
point $\langle H, w\rangle$, we replace it by its vector rejection and rescale
it to unit magnitude:
\begin{align*}
  H' & = \text{rej}_{\hat p}(H) / \|\text{rej}_{\hat p}(H)\|\\
  w' & = \text{rej}_{\hat p}(w) / \|\text{rej}_{\hat p}(w)\|
\end{align*}
A new classifier trained on the $\langle H', w'\rangle$ data must learn
a very different decision plane than $\hat p$, as $\hat p$ is no longer present
in any data points. This new classifier will perform strictly worse than the
original, otherwise the first classifier would have learned this hyperplane.
Nonetheless, it will be able to learn {\em new} H-features which the
original classifier was unable to capture. By repeating this process several
times, we can find several H-feature detectors, $\hat p_1, \ldots, \hat p_n$.

In each iteration $i$ of the procedure, we generate a four-valued feature vector
$F_i$, based on the H-feature detector $\hat p_i$. Each
feature vector contains (1) the similarity of $H_i$ and $w_i$ (before projection);
(2) the H-feature detector
$\hat p_i$ applied to $H_i$; (3) the H-feature detector $\hat p_i$ applied to $w_i$; and
(4) the difference of 2 and 3.
\begin{align*}
  & F_i(\langle H_i, w_i\rangle, \hat p_i)\\
  & \qquad = \langle H_i^{\top}w~~;~~H_i^\top\hat p_i~~;~~w_i^\top\hat p_i~~;~~H_i^\top\hat p_i - w_i^\top\hat p_i\rangle
\end{align*}
These four ``meta''-features capture all the benefits of the H-feature
detector (slots 2 and 3), while addressing Concat's issues with
similarity arguments (slot 1) {\em and} distributional inclusion (slot 4).

The union of all the feature vectors $F_1, \ldots, F_n$ from repeated iteration form a
$4n$-dimensional feature vector which we use as input to another classifier.
This classifier is trained on the exact same training data as each of the
individual Hearst Pattern detectors, so the procedure only acts as a method of
feature extraction. We use an SVM with an RBF-kernel, as we found it to work
best, though several nonlinear classifiers also perform well.

\begin{table}
\centering
\begin{small}
\begin{tabular}{|l|rrrr|}
  \hline
  Model            &      LEDS   &      BLESS  &      Medical  &      TM14   \\
  \hline
  \hline
  \multicolumn{5}{|c|}{Linear Models}\\
  \hline
  Cosine only (Baseline)              &      .787   &      .208   &      .168     &      .676   \\
  Concat                              &      .794   &      .612   &      .218     &      .693   \\
  Diff \cite{weeds:2014:coling}       &      .805   &      .440   &      .195     &      .665   \\
  Asym \cite{roller:2014:coling}      &      .865   &      .510   &      .210     &      .671   \\
  %Concat+Diff                        &      .801   &      .604   &      .224     &      .703   \\
  Asym + Concat \cite{beltagy:2016:cl}&      .843   &  {\bf.631}  &      .240     &      .701   \\
  \hline
  \multicolumn{5}{|c|}{Nonlinear Models}\\
  \hline
  RBF                                         &      .779   &      .574   &      .215     &      .705   \\
  Ksim \cite{levy:2014:conll}                 &      .893   &      .488   &      .224     &  {\bf.707}  \\
  H-Feature Detector \cite{roller:2016:emnlp} &  {\bf.901}  &  {\bf.631}  &  {\bf.260}    &      .697   \\
  \hline
\end{tabular}
\end{small}
\caption{Mean F1 scores for each model and data set.}
\label{tab:hfeatureresults}
\end{table}

We compare our H-feature detector model to several existing and alternative
baselines from the literature. Namely, we include a baseline Cosine classifier,
which only learns a threshold which maximizes F1 score on the training set;
three linear models of prior work, Concat, Diff and Asym; and the RBF and Ksim
models found to be successful in \newcite{kruszewski:2015:tacl} and
\newcite{levy:2015:naacl} respectively. We also include
Asym + Concat, which was used in \newcite{beltagy:2016:cl}. We cannot include a
additional comparisons like Ksim+Asym, because Ksim is based on a custom SVM
kernel which is not amenable to combinations.

Table~\ref{tab:hfeatureresults} the results across all four data sets for all
of the listed models. Our H-Feature model improves
significantly\footnote{Bootstrap test, $p<.01$.} over Concat in the LEDS, BLESS
and Medical data sets, indicating the benefits of combining these the aspects
of similarity and distributional inclusion with the H-feature detectors of
Concat.  The Asym + Concat classifier also improves over the Concat baseline,
further emphasizing these benefits. Our H-feature model performs approximately
the same as Ksim on the LEDS and TM14 data sets (no significant difference),
while significantly outperforming it on BLESS and Medical data sets.



\subsection{Lexical Entailment}

\paragraph{H-features for non-Hypernymy Relations}

In Section~\ref{sec:hfeatures}, we discussed how certain distributional models
act as {\em H-feature} detectors, which identify contexts highly indicative
of hypernymy, and discuss our model which exploits
multiple H-feature detectors in order to improve the
modeling power of a hypernymy detection system, and improves results over
comparable models.

However, there are many relations {\em other} than hypernymy which are useful
in considering textual entailment: for example, identifying co-hyponymy is
useful as a negative signal for entailment, and identifying meronomy is
critical to our motivating example in the Introduction.
Indeed, the results shown in Table~\ref{tab:asymresults} show
the accuracy of the Asym and Baroni classifiers on a {\em four-way} relationship
prediction task: hypernymy, co-hyponymy, meronomy, and random, but the
experiments in Table~\ref{tab:hfeatureresults} only describe performance in a
binary hypernymy-or-not classification task. We propose to extend and evaluate
the H-features model of Section~\ref{sec:hfeatures} to handle non-hypernymy
relations. We believe the model's performance can be improved by
better modeling the non-hypernymy cases, and that the model will additionally
discover H-features indicative of other relations.

There are several ways that the model could be extended. The one we believe
will be most successful is one that trains several binary H-features models:
one for hypernymy-vs-non-hypernymy, one for meronomy-vs-non-meronomy, etc.
Similar to how the PCA procedure was used only as a form of feature-extraction
for the final prediction, each of the binary classifier iterations will be also
used for feature extraction for a final classifier.
That is, we will use the procedure described in Section~\ref{sec:hfeatures}
to extract several iterations of features for hypernymy, then completely repeat
the procedure for meronomy and so forth. The resulting features from each
of the classifiers will be concatenated for a final four-way classifier prediction.
Another alternative would be to try to learn the four-way classifiers concurrently
(e.g., a softmax instead of logistic regression), and extract the corresponding
H-features at this level.

There are interesting research questions that stem from this procedure, beyond
just final performance scores. One is what distributional-level features will
be learned as prototypical of meronyms, or co-hyponyms? As we saw in
Table~\ref{tab:ctxsim}, the classifier automatically learned to pick
out well-known Hearst patterns, indicative of hypernymy. It remains to be seen
whether it will pick out additional Hearst patterns indicative of other
relations: for example, simple conjunctions for co-hyponymy (e.g., {\em cats
and dogs}) or the possessive for meronomy (e.g., {\em cat's tail}).


