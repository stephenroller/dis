\chapter{Literature Review Notes}

{This section isn't to go in the dissertation, but rather is notes
for myself.}

\section{Hypernymy detection}

\cite{nayak:2015:techreport} Neha does some stuff

\cite{vylomova:2016:acl} Need to update summary

\cite{seitner:2016:lrec}: Large extraction of hypernymy with small context (noun
modifiers). Using hearst patterns over common crawl. Uses POS tags to embed
noun phrases.

\cite{sang:2007:acl}: Short paper. Dutch hypernymy detection. Simple web
queries over hearst patterns, plus some morphology; emphasis on using the head
of dutch compounds (like german ones) to help detect hypernyms.

\cite{ritter:2009:aaai}: Basic features (hearst patterns) plus an SVM. Also an
HMM language model's state as extra features.

\section{RTE}

\cite{bowman:2016:acl}: Fast unified model for parsing and understanding
sentences.

\cite{mou:2016:acl}: Attempts SNLI using tree-based convolution nets. They're
not as cool as you'd like.

\section{Other}

\cite{qiu:2016:emnlp}: WSI by E-M with the skip gram objective and a markov
assumption over the sentence. Cheats by using gold number of senses. Does
slightly better than stuff from Andrew Mccallum.

\cite{kiddon:2016:emnlp}: Neural Checklist models for better generation.


