\chapter{To Delete: Long Term Proposals}

We now describe some possible longer term research questions which could be
addressed in the final thesis. Success in any of these items would
be significant contributions to the field, and are therefore more ambitious and
risky than the short-term proposals.

\subsection{Ontology Constraints in Hypernymy Prediction}

Presently in our hypernymy-prediction models, relationships between all
pairs of words are made independently: the prediction of whether ``cat is an
animal'' has no bearing on whether ``dog is an animal,'' aside from their
distributional similarity. While this is a straightforward application of
machine learning principles to the problem, it ignores an important fact: that
hypernymy is just one aspect of a complete, well-structured ontology.
Yet, since we predict each word pair individually, there is no
guarantee the output of the system over all pairs will also be
well-structured.

For example, a hypernymy prediction model could predict that both ``animal is a
hypernym of dog'' and that ``dog is a hypernym of animal,'' even though we know
hypernymy is non-reflexive. Or it could predict that ``a golden retriever is a
dog'' and ``dog is an animal,'' but incorrectly predict that ``gold retriever is
{\em not} an animal,'' violating the transitive property of hypernymy. These
properties of hypernymy are inherent to its definition, and our models
should be take this into account.

With this in mind, is it possible to modify our models such that such
ontological constraints are guaranteed or preferred? One
possibility would be to use the {\em confidence scores} associated with the
hypernymy predictions, and simply revise the least-confident predictions to
conform to constraints in a post-processing step. For example, in our reflexive example
above, the more confident of the two predictions will be assumed to be
the correct one.

This idea could likely benefit further from our short-term proposal to
see how well the H-features model does at predicting relations other
than hypernymy: for example, if we are highly confident that two words are
co-hyponyms, then we can become more confident that they share a common
hypernym, and vice versa.

This idea of enforcing ontological constraints is not new: it was previously
explored by \newcite{caraballo:1999:acl}, who use a clustering algorithm in
order to find co-hyponym terms, and then predict hypernyms using the entire
clusters. It was also examined some in \newcite{snow:2004:nips}, who used
syntactic Hearst Patterns to make predictions about hypernymy, and then
linearly interpolated the prediction confidences in order to better
conform to these hard constraints. Later, \newcite{snow:2006:acl}
proposed a probabilistic model over {\em ontologies}, and
an algorithm for searching over entire ontologies constrained by rules about
the transitivity of hypernymy, and reflexivity of co-hyponymy. This ontology
search is then used in order to find the single ontology which has maximal
probability according to the evidence provided by Hearst patterns, while also
not violating any of the hard constraints.

Although \newcite{snow:2006:acl} found their ontology searching algorithm
highly successful with the use of the lexico-syntactic patterns indicative
of hypernymy, such an approach is yet to be tried on classifiers which make
use of {\em distributional information} about words. Therefore, the most reasonable
first course of action would to reimplement and examine whether their model
is compatible with the distributional models of hypernymy prediction.
However, their model supports only
two forms of constraints (transitivity of hypernymy and reflexivity of
co-hyponyms), leaving open questions about how other constraints should be
imposed.

Another possibility is through the use of the same technology powering the
end-to-end RTE system of \newcite{beltagy:2016:cl}: Markov Logic Networks
(MLNs).  Markov Logic Networks provide a framework for doing probabilistic
logical inference over sets of weighted atoms and logical rules. MLNs are given
a set of weighted First Order Logic (FOL) rules and a database of atoms, and
give reweighted predictions about the probabilities of atoms based on their
consistency with the logical rules. MLNs can encode many
different relations as rules, and perform joint updating of {\em all} lexical
relationship predictions.  For example, the rules for transitivity and reflexivity
discussed in \newcite{snow:2006:acl} could be encoded as:
\begin{align*}
  \forall x,y,z. &~\text{hyper}(x,y) \wedge \text{hyper}(y, z) \rightarrow \text{hyper}(x, z),\\
  \forall x,y. &~\text{cohyp}(x,y) \leftrightarrow \text{cohyp}(y,x),
\end{align*}
but other rules, such that co-hyponyms share a hypernym, may also be encoded:
\begin{align*}
  \forall x,y,z. &~\text{cohyp}(x,y) \wedge \text{hyper}(x,z) \rightarrow \text{hyper}(y,z).
\end{align*}

MLNs ability to incorporate {\em weighted rules} would also give room for
flexibility in constraint importance, and allow for {\em some} violations of
constraints when the evidence is simply overwhelming. Therefore, we believe
both the model of \newcite{snow:2006:acl} and an MLN-based model to be strong
candidates for improving lexical relationship prediction by enforcing ontology
constraints.

\subsection{Sophisticated Contexts for Lexical Substitution}

\paragraph{Wider Contexts in Lexical Substitution}
In the model of Lexical Substitution discussed in Section~\ref{sec:pic}, we
showed how the syntactic neighbors of a target word are useful in predicting
what are the lexical substitutes, or in-context synonyms, of the target word.
Although the syntactic neighbors can indeed capture some kinds of {\em long-distance
dependencies}, there is much greater deal of context available which is
presently not used by the model: the {\em entire rest of the sentence}.

Consider if our PIC model were asked to find the best verb
to fill-in-the-blank in the following two simple sentences:

\begin{center}
  {\em The dog \_\_\_ the tennis ball.}\\
  {\em The dog \_\_\_ the meat ball.}
\end{center}

In both cases, PIC will be given the exact same context for the missing
verb: its subject should be {\em dog} and its object should be {\em ball}. However,
humans know that the dog is more likely to {\em chase} or {\em fetch} the tennis
ball, while it is more likely to {\em eat} the meat ball. Without being provided
the additional information about the ball, the model has absolutely no way
to distinguish these two cases. How can this information be integrated into our
model?

Even beyond this simple example, it is already clear from prior work that
additional context can be very useful in the task. \newcite{dinu:2010:emnlp} considers
a probabilistic model based on a wide Bag-of-Words context, and
\newcite{vandecruys:2011:emnlp} propose a model which combines
syntactic distributional space with a bag-of-words distributional space,
showing modest improvements over each individual space. Additionally,
\newcite{kawakami:2016:iclr} obtained state-of-the-art results on Lexical
Substitution using a neural network language model which encodes the {\em
entire sentence} as input, though their model also depends heavily on the use
of machine translation data. It is clear that there is more useful information
available than our own Lexical Substitution model is provided.

There two distinct ways we could implement wider contexts into our Lexical
Substitution model: using linguistic knowledge, and using neural networks.
In the former, we could simply use existing linguistic knowledge in order to
model additional syntactic patterns from large corpora. That is, in
addition to modeling the direct syntactic neighbors of words,
we could also add pattern-based rules for modeling
indirect neighbors. For example, we could introduce an additional context
in our distributional space for {\tt dobj+compound+\_+meat}, marking that the
direct object the verb is compounded with ``meat,'' and so on for other nouns.
Since our model uses collapsed prepositional phrases, this is already
partially implemented (e.g., we already model ``to the store'' as {\tt
prep:to\_store} rather than just {\tt prep:to}).

A variation of this approach was discussed in \newcite{pado:2007:cl}, the
original proposal of syntactic distributional spaces. In their model, they also
extracted contexts for dependency chains two and three hops away from the
target word, rather than the only the direct neighbors. Since then, most models
have focused mostly on direct neighbors, since longer chains substantially
increase complexity, sparsity, and model parameters.
If we use this linguistic
approach, issues of scaling and sparsity will likely plague our model.

The other possibility for this problem would be to employ modern neural network
models, like the Long Short Term Memory (LSTM) model \cite{hochreiter:1997:nc}.
The LSTM model has become extremely popular in the field of Natural Language
Processing, thanks to recent advances in network initialization
\cite{glorot:2010:aistats}, training techniques
\cite{duchi:2011:jmlr,kingma:2014:arxiv}, and hardware like Graphics Processing
Units (GPUs). Indeed, LSTMs are the basis for the successful model of
\newcite{kawakami:2016:iclr} mentioned previously. LSTMs are a form of a
Recurrent Neural Network, which takes as input a variable-length sequence of
items (tokens), and make some prediction. They have been successfully applied
countless areas of NLP, including language modeling
\cite{sundermeyer:2012:interspeech,jozefowicz:2016:arxiv}, sentiment analysis
\cite{kumar:2016:icml}, machine translation \cite{sutskever:2014:nips},
textual entailment \cite{bowman:2015:emnlp}, question answering
\cite{hermann:2015:nips}, and recently even lexical entailment
\cite{shwartz:2016:acl}.

More concretely, we could integrate wider syntax using an approach similar to
that of \newcite{shwartz:2016:acl}. In their model, a syntactic chains
connecting two target words was used to classify whether the words stand in a
hypernymy relationship or not, acting as a modern neural-network version of
\newcite{snow:2004:nips}. We propose using similar syntactic chains to simply
predict the {\em last word} in the chain, training using billions
of syntactic chains extracted from large corpora. This model would have the
capability of capturing and remembering relevant long-distance information
whenever it is helpful in predicting the final word. One challenge
in applying this model is scaling it, as LSTM models can take days or weeks
to train when their final output is a prediction over the entire vocabulary.

\paragraph{Joint Models of Syntactic Neighbors}

Another issue with the lexical substitution model discussed in Section~\ref{sec:pic}
is that the model fundamentally assumes independence between each of the syntactic
neighbors. Ultimately, the model suggests substitutes in the above examples by
asking, ``What does a dog do? What is done to a ball? What is the intersection
of these two sets?'' Our use of unnormalized inner products in Equation~\ref{eqn:pic}
means that one question may have more weight than another, but they
are still considered independently: the question is never ``What does a dog do
to a ball?''

It is worth considering whether this independence assumption can be relaxed
in some way. Intuitively, it seems obvious that a joint model should be helpful.
Yet, one major issue is that the number of syntactic neighbors is variable:
consider that a verb may have attachments may have only one attachment (intransitive),
two attachments (transitive), or more (ditransitive, prepositional phrase attachments,
adverbs, etc). Similarly, nouns can also stand in many syntactic relations
simultaneously (compound, adjective, prepositional phrase, verb relation, and
others). Since the number of attachments is variable, it is difficult to define
a probabilistic model which does not demand at least {\em some}
independence assumptions. Even if we were to define the model over all possible
syntactic relations for every word, the issue would not be solved: consider
a ``small, furry, brown mouse,'' which has three modifiers all standing
in the {\tt adjmod} relation.

Even if we could define such a model, it would likely be plagued the typical
extreme sparsity issues: for many sentences in our dataset, an {\em exact}
combination of attachments seen is unlikely to appear anywhere, even in
extremely large corpora. Therefore, a naive joint model is likely to estimate
the probability as zero for most everything.

As with the proposed methods for including wider contexts, we could potentially
address these issues using either linguistic knowledge, or neural networks.  To
address the concerns using linguistic knowledge, we could again mark certain
rules should be modeled jointly, and assume independence between the remaining
syntactic relations. For example, we could model transitive verbs jointly, but
assume independence from adverbs and prepositional modifiers; or we could
jointly model two adjective modifiers, but assume independence once we
encounter three or more. Again, rules similar to the ones discussed in
\newcite{pado:2007:cl} would be a good starting point, and others could be
proposed in qualitative analysis of the data.

The other possibility is to use LSTMs in order to jointly model the different
modifiers. Since LSTMs are able to handle a variable-length sequence as input,
they seem to be a good candidate for creating a joint model over all the
attachments in a principled manner. Unfortunately, LSTMs also assume that the
{\em order} of the sequence is important, which is not the case for our problem:
there is no inherent ordering of syntactic attachments. We could canonicalize
the ordering (simply insisting that, for example, subjects are always presented
before objects), but it remains unclear whether this is useful. We could also
{\em randomize} the order the attachments are presented: since there are many
permutations, this could substantially increase the amount of training data
available. However, preliminary
experiments showed difficulty with this second approach: the PIC model came
about when our first attempts at using LSTMs were unsuccessful.

Nonetheless, it is encouraging that the same linguistic and neural network
approaches could be potentially useful for both introducing wider context,
and joint modeling.

\subsection{Generalized Distributional Spaces for Entailment and Substitution}

Our final long term proposal is fairly different from the other two: while
those were ideas to directly impact performance on the Lexical Entailment and
Lexical Substitution, our last idea focuses more broadly on how
we can improve distributional spaces altogether.

We begin with an observation about the process described in
Section~\ref{sec:dist} when constructing a syntactic distributional space.
During construction, the final step involves finding a
matrix factorization for the extremely large, extremely sparse word-context
co-occurrence matrix. In syntactic spaces, the contexts are distinguished by
the neighboring word {\em together} with its syntactic relation. That is there
is one context for {\tt nsubj\_dog} and another for {\tt dobj\_dog}. This is a
powerful notion, and one that enables the syntactic spaces their ability
to model selectional preferences.

Yet, modeling each of these contexts independently also seems to be wasting
a large amount of information: that the same word is being modified
by two different syntactic relations. When this is ignored,
we assume we have excellent statistics for how every word stands
in every relation, but this is unrealistic, and one of the important
steps in constructing a syntactic distributional space is to limit modeling
to only the most frequent contexts. But in defining a cutoff at all, we
are also omitting a significant amount of data.
Do we really want to throw away all this data?

We ask whether the {\em syntactic relations} be separated
from the {\em words} in syntactic distributional spaces? 
Can these context vectors we rely heavily on in our work be {\em generatively
modeled}? One way to approach this would be to model it as a
composition function:
\begin{equation*}
  \vec{ {\tt nsubj\_dog} } = f({\tt nsubj}, \vec{dog}).
\end{equation*}
In this view, we would need to learn one function, $f$, which takes in a syntactic
relation and a word, and produces a {\em context vector} on how dog acts in the
subject position. One possibility would be to use vector concatenation,
treating the relation and the word as separate components, independent of each
other. Intuitively, this does not seem to capture the the essence of our
proposal, but it could act as a baseline model:
\begin{equation*}
  \vec{ {\tt nsubj\_dog} } = \langle \vec{dog} ; \vec{nsubj} \rangle.
\end{equation*}
Another possibility would be to treat the procedure as a form of post-processing
generalization. For example, we could factorize the context vectors as we do now, and
then attempt to learn to predict the observed context vectors from the input
components. For example, perhaps after performing the concatenation, a simple
linear regression could combine them:
\begin{equation*}
  \vec{ {\tt nsubj\_dog} } = W\langle \vec{dog} ; \vec{nsubj} \rangle + \vec{b}.
\end{equation*}
This is a basic model, but it would still enable us to
predict information about novel combinations of syntactic relations and words.
One could increase the representational power using a more sophisticated
regression model, like neural networks.

Another variation of this linear regression idea, would be to model each
syntactic relation as a function {\em applied} to the vector. For example,
we would learn a separate transformation for {\tt nsubj}, {\tt dobj}, etc:
\begin{equation*}
  \vec{ {\tt nsubj\_dog} } = W_{{\tt nsubj}}\vec{dog}
\end{equation*}
This gives the model many more free parameters, but may present some difficulty
for the rarest syntactic relations, where there could easily be more parameters
in $W$ than examples, though perhaps heavy regularization of $W$, or even
constraining it to be diagonal could be beneficial. Modeling the behavior in
this way would draw parallels to other areas of the literature,
like compositional distributional semantics \cite{baroni:2010:emnlp,coecke:2011:la,grefenstette:2011:emnlp,baroni:2014:li}, the
bilinear models proposed in GloVe \cite{pennington:2014:emnlp}, and the recursive
composition models for sentiment analysis \cite{socher:2013:emnlp}.

Alternatively, we could  try to learn the distributional vectors from scratch
using an alternative factorization. Recall that the dimensionality reduction procedure
simply tries to predict the Pointwise Mutual Information between a word and
a context. Rather than performing the dimensionality reduction, then regression,
we could encode our generative principals into the dimensionality reduction.
In this way, a successful model may look something more like {\em tensor
factorization}. For example, we could model the PMI between a word ($v$),
and a relation-context pair ($r, c$) as:
\begin{equation*}
  PMI(v, r, c) = {\bf v}^\top {\bf W}_{r} {\bf c},
\end{equation*}
where ${\bf W_r}$ is a matrix. This is essentially the same as the linear
regression model proposed in the above paragraph, but performed at a different
step. Unfortunately, tensor factorization is generally very difficult
\cite{haastad:1990:ja,hillar:2013:jacm}. Finding the ideal
factorization for our problem is likely a very difficult optimization problem
without major simplifying assumptions, and beyond our own expertise.

However, an excellent place to start would be in the Information Extraction
literature, especially those focused on Statistical Relational Learning. These
works seek to find generative representations for (subject, verb, object)
triples, like {\tt BornIn(John, Athens)}, and there is a rich literature
regarding approaches to this very difficult problem.
The approach of \newcite{nickel:2011:icml} inspired the model described
in the previous paragraph, but many other models have been proposed
\cite{socher:2013:nips,riedel:2013:naacl,yang:2014:iclr,kuleshov:2015:aistats,trouillon:2016:icml}.

Regardless, if we assume that we {\em can} find any model which successfully
predicts context vectors in a generative manner, then it could lead to
substantial improvements on both tasks of Lexical Entailment {\em and} Lexical
Substitution. For example, we saw in Table~\ref{tab:ctxsim} that certain
syntactic relations are indicative of hypernymy. Therefore, being able to plug
in {\em any} two words to and estimate the likelihood that they will stand in this
syntactic relation, like {\tt nmod:such\_as}.
The problem would contribute even more greatly to the Lexical Substitution
model we presented: in a preliminary examination, we found that some 18\% of the Coinco
dataset contains at least one context not available in our distributional space.
The ability to {\em generate} representations for these contexts would be
a simple way to provide the model with more disambiguating information, which
should help in the end task.

Indeed, as we described above, a novel model could easily have larger
implications in other areas of Natural Language Processing, and is therefore
probably the most ambitious and difficult of our long term proposals.


